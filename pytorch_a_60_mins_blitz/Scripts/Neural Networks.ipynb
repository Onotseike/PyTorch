{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37764bite650467ef4694640988010b9b7823d44",
   "display_name": "Python 3.7.7 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Networks\n",
    "\n",
    "# Training Procedure\n",
    "\n",
    "## 1. Define the Neural Networks that has some learnable parameters ( or weights).\n",
    "## 2. Iterate over the Dataset of inputs.\n",
    "## 3. Process Input through the network.\n",
    "## 4. Compute the loss (How far is the output from being correct).\n",
    "## 5. Propagate gradients back into the Network's Parameters\n",
    "## 6. Update the weights of the network, typically using a simple update rule: WEIGHT = WEIGHT - LEARNING_RATE * GRADIENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statemnts\n",
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as fxnl\n",
    "import torch.optim as opt\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Define the Neural Network\n",
    "\n",
    "class  NeuralNet(nn.Module):\n",
    "\n",
    "    ## Constructor(s)\n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__()\n",
    "\n",
    "        # 2D Convolution Layer with 1 input channel, 6 output channels and a convolution kernel size(3*3)\n",
    "        self.conv_layer_one = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=3) \n",
    "\n",
    "        # 2D Convolution Layer with (conv_layer_one.out_channel) input channels, 16 output channels and kernel size (3*3)\n",
    "        self.conv_layer_two = nn.Conv2d(in_channels= 6, out_channels= 16, kernel_size=3)\n",
    "\n",
    "        self.fc_one = nn.Linear(in_features=16 * 6 * 6, out_features=120)\n",
    "        self.fc_two = nn.Linear(in_features=120, out_features=84)\n",
    "        self.fc_three = nn.Linear(in_features=84, out_features=10)\n",
    "    \n",
    "    # Forwarding Method\n",
    "    def forward(self, forward_linker):\n",
    "        #Max Pooling over a (2, 2) window\n",
    "        forward_linker = fxnl.max_pool2d(fxnl.relu(self.conv_layer_one(forward_linker)), (2, 2))\n",
    "\n",
    "        forward_linker = fxnl.max_pool2d(fxnl.relu(self.conv_layer_two(forward_linker)), 2)\n",
    "        forward_linker = forward_linker.view(-1, self.num_flat_features(forward_linker))\n",
    "\n",
    "        forward_linker = fxnl.relu(self.fc_one(forward_linker))\n",
    "        forward_linker = fxnl.relu(self.fc_two(forward_linker))\n",
    "        forward_linker = self.fc_three(forward_linker)\n",
    "\n",
    "        return forward_linker\n",
    "\n",
    "    # Helper Method(s)\n",
    "    def num_flat_features(self, linker):\n",
    "        size = linker.size()[1:]\n",
    "        num_features = 1\n",
    "\n",
    "        for val in size:\n",
    "            num_features *= val\n",
    "        \n",
    "        return num_features\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "NeuralNet(\n  (conv_layer_one): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n  (conv_layer_two): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n  (fc_one): Linear(in_features=576, out_features=120, bias=True)\n  (fc_two): Linear(in_features=120, out_features=84, bias=True)\n  (fc_three): Linear(in_features=84, out_features=10, bias=True)\n)\n"
    }
   ],
   "source": [
    "neural_net = NeuralNet()\n",
    "print(neural_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "10\ntorch.Size([6, 1, 3, 3])\ntorch.Size([6])\ntorch.Size([16, 6, 3, 3])\ntorch.Size([16])\ntorch.Size([120, 576])\ntorch.Size([120])\ntorch.Size([84, 120])\ntorch.Size([84])\ntorch.Size([10, 84])\ntorch.Size([10])\n"
    }
   ],
   "source": [
    "parameters = list(neural_net.parameters())\n",
    "print(len(parameters))\n",
    "for id in range(len(parameters)):\n",
    "    print(parameters[id].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[[[-0.4921,  1.5270,  0.1252,  ...,  0.3663,  1.4028,  1.4076],\n          [ 0.0825, -1.6777, -1.8811,  ...,  0.2698,  1.5417, -0.2626],\n          [ 0.1157, -1.9528,  0.0195,  ...,  0.7288, -0.1998,  0.4658],\n          ...,\n          [ 0.2175, -0.5989, -0.1006,  ...,  1.2585,  0.4564, -0.4092],\n          [-0.2639,  1.6002,  0.3030,  ..., -0.1700, -0.6620,  0.0823],\n          [ 0.3655, -2.0442,  0.9534,  ..., -0.4443,  0.3536,  0.5260]]]])\n"
    }
   ],
   "source": [
    "# Random input \n",
    "rand_input = torch.randn(1, 1, 32, 32)\n",
    "print(rand_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[ 0.0357,  0.0424,  0.0950, -0.0865,  0.1682, -0.0839,  0.0280, -0.0366,\n         -0.0928,  0.0673]], grad_fn=<AddmmBackward>)\n"
    }
   ],
   "source": [
    "rand_output = neural_net(rand_input)\n",
    "print(rand_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_net.zero_grad()\n",
    "rand_output.backward(torch.randn(1, 10), retain_graph= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor(0.9651, grad_fn=<MseLossBackward>) 0.9651324152946472\n"
    }
   ],
   "source": [
    "# Loss Function\n",
    "target_output = torch.randn(10).view(1, -1)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "loss = criterion(rand_output, target_output)\n",
    "print(loss, loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<MseLossBackward object at 0x0000015A4CAEE8C8>\n<AddmmBackward object at 0x0000015A4CAEE8C8>\n<AccumulateGrad object at 0x0000015A4CAD4888>\n"
    }
   ],
   "source": [
    "print(loss.grad_fn)  # MSELoss\n",
    "print(loss.grad_fn.next_functions[0][0])  # Linear\n",
    "print(loss.grad_fn.next_functions[0][0].next_functions[0][0])  # ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "conv1.bias.grad before backward\ntensor([0., 0., 0., 0., 0., 0.])\nconv1.bias.grad after backward\ntensor([-0.0127,  0.0055,  0.0026,  0.0027, -0.0030, -0.0136])\n"
    }
   ],
   "source": [
    "# Backprop\n",
    "neural_net.zero_grad()\n",
    "print('conv1.bias.grad before backward')\n",
    "print(neural_net.conv_layer_one.bias.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('conv1.bias.grad after backward')\n",
    "print(neural_net.conv_layer_one.bias.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update Weights\n",
    "# SGD = Stochastic Gradient Descent\n",
    "## W = W - LR * G\n",
    "\n",
    "learning_rate = 0.01\n",
    "for feature in neural_net.parameters():\n",
    "    feature.data.sub_(learning_rate * feature.grad.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# However, as you use neural networks, you want to use various different update rules such as SGD, Nesterov-SGD, Adam, RMSProp, etc. To enable this, we built a small package: torch.optim that implements all these methods. Using it is very simple:\n",
    "\n",
    "optimizer = opt.SGD(neural_net.parameters(), lr=0.01) ## used different types\n",
    "\n",
    "### In your training loop \n",
    "optimizer.zero_grad() ## clear graidient buffers\n",
    "output = neural_net(rand_input)\n",
    "loss = criterion(output, target_output)\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}