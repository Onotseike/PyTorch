{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37764bite650467ef4694640988010b9b7823d44",
   "display_name": "Python 3.7.7 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statements\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch Loss pair 0 32570906.456289362\n\nEpoch Loss pair 1 24919643.695544064\n\nEpoch Loss pair 2 18871516.72931711\n\nEpoch Loss pair 3 13384847.969570603\n\nEpoch Loss pair 4 8978487.885150198\n\nEpoch Loss pair 5 5886105.268144172\n\nEpoch Loss pair 6 3927536.953626305\n\nEpoch Loss pair 7 2733028.317743892\n\nEpoch Loss pair 8 2001537.0453600758\n\nEpoch Loss pair 9 1536565.8125415863\n\nEpoch Loss pair 10 1223849.0550200008\n\nEpoch Loss pair 11 1001821.2689651926\n\nEpoch Loss pair 12 835898.3228042189\n\nEpoch Loss pair 13 706961.5482111586\n\nEpoch Loss pair 14 603899.5674323032\n\nEpoch Loss pair 15 519861.3318611538\n\nEpoch Loss pair 16 450271.5285312706\n\nEpoch Loss pair 17 392101.4799038395\n\nEpoch Loss pair 18 343087.00791163533\n\nEpoch Loss pair 19 301343.01322013367\n\nEpoch Loss pair 20 265594.5547427781\n\nEpoch Loss pair 21 234848.9862650811\n\nEpoch Loss pair 22 208259.86564860446\n\nEpoch Loss pair 23 185178.412069844\n\nEpoch Loss pair 24 165070.95301717674\n\nEpoch Loss pair 25 147506.434906177\n\nEpoch Loss pair 26 132123.30399332056\n\nEpoch Loss pair 27 118600.43930039488\n\nEpoch Loss pair 28 106672.39256223978\n\nEpoch Loss pair 29 96122.97856820966\n\nEpoch Loss pair 30 86787.7098529841\n\nEpoch Loss pair 31 78501.35247579696\n\nEpoch Loss pair 32 71122.47741200904\n\nEpoch Loss pair 33 64527.05628160639\n\nEpoch Loss pair 34 58630.85156096155\n\nEpoch Loss pair 35 53350.86420674611\n\nEpoch Loss pair 36 48615.91915541023\n\nEpoch Loss pair 37 44363.17518639384\n\nEpoch Loss pair 38 40534.08294500933\n\nEpoch Loss pair 39 37081.50958975374\n\nEpoch Loss pair 40 33962.80463118413\n\nEpoch Loss pair 41 31142.234671662383\n\nEpoch Loss pair 42 28588.16940059964\n\nEpoch Loss pair 43 26270.752030842174\n\nEpoch Loss pair 44 24168.389453177962\n\nEpoch Loss pair 45 22256.374914478874\n\nEpoch Loss pair 46 20516.31094815983\n\nEpoch Loss pair 47 18931.068909099973\n\nEpoch Loss pair 48 17486.673489094395\n\nEpoch Loss pair 49 16167.29905676123\n\nEpoch Loss pair 50 14961.216310995036\n\nEpoch Loss pair 51 13857.30249991513\n\nEpoch Loss pair 52 12846.36338498472\n\nEpoch Loss pair 53 11919.138296138932\n\nEpoch Loss pair 54 11067.796860877652\n\nEpoch Loss pair 55 10285.636442583478\n\nEpoch Loss pair 56 9566.471674099474\n\nEpoch Loss pair 57 8904.451135231562\n\nEpoch Loss pair 58 8294.590624879937\n\nEpoch Loss pair 59 7732.502314987787\n\nEpoch Loss pair 60 7213.707645954979\n\nEpoch Loss pair 61 6734.331121184932\n\nEpoch Loss pair 62 6291.083345802797\n\nEpoch Loss pair 63 5880.906466354147\n\nEpoch Loss pair 64 5501.091679939819\n\nEpoch Loss pair 65 5149.025479771592\n\nEpoch Loss pair 66 4822.559660592287\n\nEpoch Loss pair 67 4519.460441177209\n\nEpoch Loss pair 68 4238.428091633088\n\nEpoch Loss pair 69 3977.1738246452264\n\nEpoch Loss pair 70 3734.08172224993\n\nEpoch Loss pair 71 3507.791333637781\n\nEpoch Loss pair 72 3297.0345913035853\n\nEpoch Loss pair 73 3100.637838610495\n\nEpoch Loss pair 74 2917.4725867884736\n\nEpoch Loss pair 75 2746.505108741564\n\nEpoch Loss pair 76 2586.830962330418\n\nEpoch Loss pair 77 2437.7122691336403\n\nEpoch Loss pair 78 2298.3250684807113\n\nEpoch Loss pair 79 2167.9092982698767\n\nEpoch Loss pair 80 2045.7845200643435\n\nEpoch Loss pair 81 1931.427162818021\n\nEpoch Loss pair 82 1824.2259949563531\n\nEpoch Loss pair 83 1723.7230980887166\n\nEpoch Loss pair 84 1629.4394251324188\n\nEpoch Loss pair 85 1540.912953841484\n\nEpoch Loss pair 86 1457.7763297788351\n\nEpoch Loss pair 87 1379.6735171983635\n\nEpoch Loss pair 88 1306.2681822829572\n\nEpoch Loss pair 89 1237.2159551566215\n\nEpoch Loss pair 90 1172.2194781691674\n\nEpoch Loss pair 91 1111.011250326531\n\nEpoch Loss pair 92 1053.3630784119555\n\nEpoch Loss pair 93 999.0572356652519\n\nEpoch Loss pair 94 947.8452974724441\n\nEpoch Loss pair 95 899.5471561130759\n\nEpoch Loss pair 96 853.9727018843719\n\nEpoch Loss pair 97 810.9738047246015\n\nEpoch Loss pair 98 770.3423054591215\n\nEpoch Loss pair 99 731.9760040829585\n\nEpoch Loss pair 100 695.7104864482234\n\nEpoch Loss pair 101 661.4122316112514\n\nEpoch Loss pair 102 628.973512918432\n\nEpoch Loss pair 103 598.2780880180953\n\nEpoch Loss pair 104 569.2361909481601\n\nEpoch Loss pair 105 541.7357688339175\n\nEpoch Loss pair 106 515.6945573617393\n\nEpoch Loss pair 107 491.0315266658376\n\nEpoch Loss pair 108 467.64153703023055\n\nEpoch Loss pair 109 445.4655776358479\n\nEpoch Loss pair 110 424.42850814489066\n\nEpoch Loss pair 111 404.47247028958446\n\nEpoch Loss pair 112 385.54081549950655\n\nEpoch Loss pair 113 367.5697494502599\n\nEpoch Loss pair 114 350.5058556062676\n\nEpoch Loss pair 115 334.2973137434962\n\nEpoch Loss pair 116 318.8965942205309\n\nEpoch Loss pair 117 304.26168918968403\n\nEpoch Loss pair 118 290.3614497678112\n\nEpoch Loss pair 119 277.14622509522036\n\nEpoch Loss pair 120 264.5772509405763\n\nEpoch Loss pair 121 252.61206916406826\n\nEpoch Loss pair 122 241.22606995306685\n\nEpoch Loss pair 123 230.39081519547653\n\nEpoch Loss pair 124 220.0766207806072\n\nEpoch Loss pair 125 210.2602603605428\n\nEpoch Loss pair 126 200.90779100442523\n\nEpoch Loss pair 127 192.00609864695315\n\nEpoch Loss pair 128 183.52138228532647\n\nEpoch Loss pair 129 175.43474151483383\n\nEpoch Loss pair 130 167.72711869336612\n\nEpoch Loss pair 131 160.37836148903875\n\nEpoch Loss pair 132 153.37129316044377\n\nEpoch Loss pair 133 146.69108969095515\n\nEpoch Loss pair 134 140.32063358947897\n\nEpoch Loss pair 135 134.24332588500977\n\nEpoch Loss pair 136 128.44366630726856\n\nEpoch Loss pair 137 122.90739280009984\n\nEpoch Loss pair 138 117.62103123655558\n\nEpoch Loss pair 139 112.57553073083594\n\nEpoch Loss pair 140 107.75841515709794\n\nEpoch Loss pair 141 103.1588522944022\n\nEpoch Loss pair 142 98.76578649634092\n\nEpoch Loss pair 143 94.569100312447\n\nEpoch Loss pair 144 90.55854575747543\n\nEpoch Loss pair 145 86.72919984269728\n\nEpoch Loss pair 146 83.0690271042201\n\nEpoch Loss pair 147 79.56940063865682\n\nEpoch Loss pair 148 76.22439791111631\n\nEpoch Loss pair 149 73.02664427874889\n\nEpoch Loss pair 150 69.9699854592195\n\nEpoch Loss pair 151 67.04500792470866\n\nEpoch Loss pair 152 64.24802533010046\n\nEpoch Loss pair 153 61.57337499308148\n\nEpoch Loss pair 154 59.015563402037216\n\nEpoch Loss pair 155 56.56836978770983\n\nEpoch Loss pair 156 54.22592507090195\n\nEpoch Loss pair 157 51.985373209736174\n\nEpoch Loss pair 158 49.83996132077523\n\nEpoch Loss pair 159 47.78657398002691\n\nEpoch Loss pair 160 45.82116155526889\n\nEpoch Loss pair 161 43.93942665034692\n\nEpoch Loss pair 162 42.138460439261195\n\nEpoch Loss pair 163 40.41352734205057\n\nEpoch Loss pair 164 38.763365838736675\n\nEpoch Loss pair 165 37.1815567195567\n\nEpoch Loss pair 166 35.66621843383625\n\nEpoch Loss pair 167 34.21471048052023\n\nEpoch Loss pair 168 32.82411388383071\n\nEpoch Loss pair 169 31.492144922828214\n\nEpoch Loss pair 170 30.216160759876406\n\nEpoch Loss pair 171 28.993989285355507\n\nEpoch Loss pair 172 27.822672231725342\n\nEpoch Loss pair 173 26.700362229044877\n\nEpoch Loss pair 174 25.624646339105105\n\nEpoch Loss pair 175 24.59306775330708\n\nEpoch Loss pair 176 23.60408660487579\n\nEpoch Loss pair 177 22.65601136693524\n\nEpoch Loss pair 178 21.747685936754017\n\nEpoch Loss pair 179 20.876844043752822\n\nEpoch Loss pair 180 20.041741406215337\n\nEpoch Loss pair 181 19.240725730381076\n\nEpoch Loss pair 182 18.47303486790539\n\nEpoch Loss pair 183 17.73632978333263\n\nEpoch Loss pair 184 17.02964985526567\n\nEpoch Loss pair 185 16.352134446403948\n\nEpoch Loss pair 186 15.70263204123065\n\nEpoch Loss pair 187 15.079178686549193\n\nEpoch Loss pair 188 14.48104768872317\n\nEpoch Loss pair 189 13.907291663823386\n\nEpoch Loss pair 190 13.356816737783927\n\nEpoch Loss pair 191 12.828820081572337\n\nEpoch Loss pair 192 12.32212496853965\n\nEpoch Loss pair 193 11.836092461793907\n\nEpoch Loss pair 194 11.369330291144518\n\nEpoch Loss pair 195 10.921382603302352\n\nEpoch Loss pair 196 10.491632725626957\n\nEpoch Loss pair 197 10.079015371558018\n\nEpoch Loss pair 198 9.682941461247768\n\nEpoch Loss pair 199 9.302914270249133\n\nEpoch Loss pair 200 8.938359176386474\n\nEpoch Loss pair 201 8.588326561424264\n\nEpoch Loss pair 202 8.252130685912954\n\nEpoch Loss pair 203 7.929402667828232\n\nEpoch Loss pair 204 7.619392990295934\n\nEpoch Loss pair 205 7.32174885275365\n\nEpoch Loss pair 206 7.036040118367138\n\nEpoch Loss pair 207 6.761798542018253\n\nEpoch Loss pair 208 6.498435216957565\n\nEpoch Loss pair 209 6.2454539118758525\n\nEpoch Loss pair 210 6.002558024049085\n\nEpoch Loss pair 211 5.7692136651811525\n\nEpoch Loss pair 212 5.545104264328929\n\nEpoch Loss pair 213 5.329907440524946\n\nEpoch Loss pair 214 5.123256339084747\n\nEpoch Loss pair 215 4.924712300871715\n\nEpoch Loss pair 216 4.733942050996621\n\nEpoch Loss pair 217 4.5506909812219885\n\nEpoch Loss pair 218 4.374681393693643\n\nEpoch Loss pair 219 4.205624488769661\n\nEpoch Loss pair 220 4.043310988587843\n\nEpoch Loss pair 221 3.8872542648031763\n\nEpoch Loss pair 222 3.7373308731146104\n\nEpoch Loss pair 223 3.593218134888816\n\nEpoch Loss pair 224 3.4547551765484634\n\nEpoch Loss pair 225 3.321776430940674\n\nEpoch Loss pair 226 3.1939979295305387\n\nEpoch Loss pair 227 3.071170043188853\n\nEpoch Loss pair 228 2.953169006555054\n\nEpoch Loss pair 229 2.839800651878249\n\nEpoch Loss pair 230 2.7308220377497205\n\nEpoch Loss pair 231 2.6260633845489982\n\nEpoch Loss pair 232 2.525406206733294\n\nEpoch Loss pair 233 2.4286282063635194\n\nEpoch Loss pair 234 2.3356193445503655\n\nEpoch Loss pair 235 2.246231936882684\n\nEpoch Loss pair 236 2.1603443095462906\n\nEpoch Loss pair 237 2.0777481786217957\n\nEpoch Loss pair 238 1.99839609331715\n\nEpoch Loss pair 239 1.9220925197255494\n\nEpoch Loss pair 240 1.8487425548865777\n\nEpoch Loss pair 241 1.7781990965706056\n\nEpoch Loss pair 242 1.7104064139864863\n\nEpoch Loss pair 243 1.6452433219078122\n\nEpoch Loss pair 244 1.5826027802505411\n\nEpoch Loss pair 245 1.522371075618616\n\nEpoch Loss pair 246 1.4644482923835778\n\nEpoch Loss pair 247 1.4087469348755244\n\nEpoch Loss pair 248 1.3551954423981145\n\nEpoch Loss pair 249 1.30372564214607\n\n"
    }
   ],
   "source": [
    "# Tensors : Warm-Up : Numpy\n",
    "# Implement the network using Numpy\n",
    "\n",
    "\n",
    "# variables\n",
    "batch_size, input_dimensions, hidden_dimensions, output_dimensions = 64, 1000, 100, 10\n",
    "learning_rate = 1e-6\n",
    "# Create random input and output data\n",
    "random_input = np.random.randn(batch_size, input_dimensions)\n",
    "random_output = np.random.randn(batch_size, output_dimensions)\n",
    "# Randomly initialize weights\n",
    "weight_one = np.random.randn(input_dimensions, hidden_dimensions)\n",
    "weight_two = np.random.randn(hidden_dimensions, output_dimensions)\n",
    "\n",
    "for idx in range(250):\n",
    "    # Perform a forward pass to compute the predicted y\n",
    "    h = random_input.dot(weight_one)\n",
    "    h_relu = np.maximum(h, 0)\n",
    "    y_pred = h_relu.dot(weight_two)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = np.square(y_pred - random_output).sum()\n",
    "    print(\"Epoch Loss pair\", idx, loss)\n",
    "    print()\n",
    "\n",
    "    # Perform a Backprop to compute gradients of weight_one and weight_two wrt loss\n",
    "    grad_y_pred = 2.0 * (y_pred - random_output)\n",
    "    grad_weight_two = h_relu.T.dot(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.dot(weight_two.T)\n",
    "    grad_h = grad_h_relu.copy()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_weight_one = random_input.T.dot(grad_h)\n",
    "\n",
    "    # Update Weights\n",
    "    weight_one -= learning_rate * grad_weight_one\n",
    "    weight_two -= learning_rate * grad_weight_two\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch Loss pair 99 378.1025390625\n\nEpoch Loss pair 199 1.3936830759048462\n\nEpoch Loss pair 299 0.008971026167273521\n\nEpoch Loss pair 399 0.00021459953859448433\n\nEpoch Loss pair 499 3.687627031467855e-05\n\n"
    }
   ],
   "source": [
    "# Implement network with PyTorch\n",
    "\n",
    "d_type = torch.float\n",
    "gpu_device = torch.device(\"cuda:0\")\n",
    "\n",
    "# variables\n",
    "batch_size, input_dimensions, hidden_dimensions, output_dimensions = 64, 1000, 100, 10\n",
    "learning_rate = 1e-6\n",
    "# Create random input and output data\n",
    "random_input = torch.randn(batch_size, input_dimensions, device=gpu_device, dtype=d_type)\n",
    "random_output = torch.randn(batch_size, output_dimensions, device=gpu_device, dtype=d_type)\n",
    "# Randomly initialize weights\n",
    "weight_one = torch.randn(input_dimensions, hidden_dimensions, device=gpu_device, dtype=d_type)\n",
    "weight_two = torch.randn(hidden_dimensions, output_dimensions, device=gpu_device, dtype=d_type)\n",
    "\n",
    "for idx in range(500):\n",
    "    # Perform a forward pass to compute the predicted y\n",
    "    h = random_input.mm(weight_one)\n",
    "    h_relu = h.clamp(min=0)\n",
    "    y_pred = h_relu.mm(weight_two)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = (y_pred - random_output).pow(2).sum().item()\n",
    "    if idx % 100 == 99:\n",
    "        print(\"Epoch Loss pair\", idx, loss)\n",
    "        print()\n",
    "\n",
    "    # Perform a Backprop to compute gradients of weight_one and weight_two wrt loss\n",
    "    grad_y_pred = 2.0 * (y_pred - random_output)\n",
    "    grad_weight_two = h_relu.t().mm(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.mm(weight_two.t())\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_weight_one = random_input.t().mm(grad_h)\n",
    "\n",
    "    # Update Weights\n",
    "    weight_one -= learning_rate * grad_weight_one\n",
    "    weight_two -= learning_rate * grad_weight_two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch Loss pair 99 941.2940673828125\n\nEpoch Loss pair 199 9.423967361450195\n\nEpoch Loss pair 299 0.16894696652889252\n\nEpoch Loss pair 399 0.004014036152511835\n\nEpoch Loss pair 499 0.00028565828688442707\n\n"
    }
   ],
   "source": [
    "# Implement Network with Tensors + AutoGrad\n",
    "\n",
    "d_type = torch.float\n",
    "gpu_device = torch.device(\"cuda:0\")\n",
    "\n",
    "# variables\n",
    "batch_size, input_dimensions, hidden_dimensions, output_dimensions = 64, 1000, 100, 10\n",
    "learning_rate = 1e-6\n",
    "# Create random input and output data\n",
    "random_input = torch.randn(batch_size, input_dimensions, device=gpu_device, dtype=d_type)\n",
    "random_output = torch.randn(batch_size, output_dimensions, device=gpu_device, dtype=d_type)\n",
    "# Randomly initialize weights\n",
    "weight_one = torch.randn(input_dimensions, hidden_dimensions, device=gpu_device, dtype=d_type, requires_grad=True)\n",
    "weight_two = torch.randn(hidden_dimensions, output_dimensions, device=gpu_device, dtype=d_type, requires_grad=True)\n",
    "\n",
    "for idx in range(500):\n",
    "    # Perform a forward pass to compute the predicted y\n",
    "    # h = random_input.mm(weight_one)\n",
    "    # h_relu = h.clamp(min=0)\n",
    "    y_pred = random_input.mm(weight_one).clamp(min=0).mm(weight_two)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = (y_pred - random_output).pow(2).sum()\n",
    "    if idx % 100 == 99:\n",
    "        print(\"Epoch Loss pair\", idx, loss.item())\n",
    "        print()\n",
    "\n",
    "    # Perform a Backprop to compute gradients of weight_one and weight_two wrt loss using AutoGrad\n",
    "    loss.backward()   \n",
    "\n",
    "    # Update Weights\n",
    "    with torch.no_grad():\n",
    "        weight_one -= learning_rate * weight_one.grad\n",
    "        weight_two -= learning_rate * weight_two.grad\n",
    "\n",
    "        # Manually zero the gradients post weight updating\n",
    "        weight_one.grad.zero_()\n",
    "        weight_two.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch Loss pair 99 536.0281372070312\n\nEpoch Loss pair199 2.800607919692993\n\nEpoch Loss pair 299 0.02065237984061241\n\nEpoch Loss pair 399 0.0003756055375561118\n\nEpoch Loss pair 499 5.1681501645362005e-05\n\n"
    }
   ],
   "source": [
    "# Implement Network with Tensors + AutoGrad + AutoGrad Function\n",
    "\n",
    "class CustomRelu(torch.autograd.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input:torch.Tensor):\n",
    "        ctx.save_for_backward(input)\n",
    "        return input.clamp(min=0)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output:torch.Tensor):\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input < 0] = 0\n",
    "        return grad_input\n",
    "\n",
    "d_type = torch.float\n",
    "gpu_device = torch.device(\"cuda:0\")\n",
    "\n",
    "# variables\n",
    "batch_size, input_dimensions, hidden_dimensions, output_dimensions = 64, 1000, 100, 10\n",
    "learning_rate = 1e-6\n",
    "# Create random input and output data\n",
    "random_input = torch.randn(batch_size, input_dimensions, device=gpu_device, dtype=d_type)\n",
    "random_output = torch.randn(batch_size, output_dimensions, device=gpu_device, dtype=d_type)\n",
    "# Randomly initialize weights\n",
    "weight_one = torch.randn(input_dimensions, hidden_dimensions, device=gpu_device, dtype=d_type, requires_grad=True)\n",
    "weight_two = torch.randn(hidden_dimensions, output_dimensions, device=gpu_device, dtype=d_type, requires_grad=True)\n",
    "\n",
    "for idx in range(500):\n",
    "    # Perform a forward pass to compute the predicted y\n",
    "    # h = random_input.mm(weight_one)\n",
    "    # h_relu = h.clamp(min=0)\n",
    "    relu = CustomRelu.apply\n",
    "    y_pred = relu(random_input.mm(weight_one)).mm(weight_two)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = (y_pred - random_output).pow(2).sum()\n",
    "    if idx % 100 == 99:\n",
    "        print(\"Epoch Loss pair\", idx, loss.item())\n",
    "        print()\n",
    "\n",
    "    # Perform a Backprop to compute gradients of weight_one and weight_two wrt loss using AutoGrad\n",
    "    loss.backward()   \n",
    "\n",
    "    # Update Weights\n",
    "    with torch.no_grad():\n",
    "        weight_one -= learning_rate * weight_one.grad\n",
    "        weight_two -= learning_rate * weight_two.grad\n",
    "\n",
    "        # Manually zero the gradients post weight updating\n",
    "        weight_one.grad.zero_()\n",
    "        weight_two.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch Loss pair 499 592.2860717773438\n\nEpoch Loss pair 499 591.8289184570312\n\nEpoch Loss pair 499 591.3723754882812\n\nEpoch Loss pair 499 590.9163208007812\n\nEpoch Loss pair 499 590.460693359375\n\nEpoch Loss pair 499 590.0059204101562\n\nEpoch Loss pair 499 589.5518188476562\n\nEpoch Loss pair 499 589.0983276367188\n\nEpoch Loss pair 499 588.645263671875\n\nEpoch Loss pair 499 588.1927490234375\n\nEpoch Loss pair 499 587.74072265625\n\nEpoch Loss pair 499 587.2892456054688\n\nEpoch Loss pair 499 586.8381958007812\n\nEpoch Loss pair 499 586.3876953125\n\nEpoch Loss pair 499 585.9376831054688\n\nEpoch Loss pair 499 585.4881591796875\n\nEpoch Loss pair 499 585.0392456054688\n\nEpoch Loss pair 499 584.5908813476562\n\nEpoch Loss pair 499 584.1429443359375\n\nEpoch Loss pair 499 583.6956176757812\n\nEpoch Loss pair 499 583.2487182617188\n\nEpoch Loss pair 499 582.8023071289062\n\nEpoch Loss pair 499 582.3565063476562\n\nEpoch Loss pair 499 581.9110717773438\n\nEpoch Loss pair 499 581.46630859375\n\nEpoch Loss pair 499 581.0222778320312\n\nEpoch Loss pair 499 580.578857421875\n\nEpoch Loss pair 499 580.1359252929688\n\nEpoch Loss pair 499 579.6934814453125\n\nEpoch Loss pair 499 579.2515258789062\n\nEpoch Loss pair 499 578.8101196289062\n\nEpoch Loss pair 499 578.369384765625\n\nEpoch Loss pair 499 577.9291381835938\n\nEpoch Loss pair 499 577.4894409179688\n\nEpoch Loss pair 499 577.05029296875\n\nEpoch Loss pair 499 576.6116333007812\n\nEpoch Loss pair 499 576.1734619140625\n\nEpoch Loss pair 499 575.7359008789062\n\nEpoch Loss pair 499 575.2987060546875\n\nEpoch Loss pair 499 574.8619384765625\n\nEpoch Loss pair 499 574.4257202148438\n\nEpoch Loss pair 499 573.989990234375\n\nEpoch Loss pair 499 573.5548095703125\n\nEpoch Loss pair 499 573.1204833984375\n\nEpoch Loss pair 499 572.6868896484375\n\nEpoch Loss pair 499 572.253662109375\n\nEpoch Loss pair 499 571.8208618164062\n\nEpoch Loss pair 499 571.3886108398438\n\nEpoch Loss pair 499 570.9573364257812\n\nEpoch Loss pair 499 570.5267333984375\n\nEpoch Loss pair 499 570.0965576171875\n\nEpoch Loss pair 499 569.6668090820312\n\nEpoch Loss pair 499 569.237548828125\n\nEpoch Loss pair 499 568.8087768554688\n\nEpoch Loss pair 499 568.3804321289062\n\nEpoch Loss pair 499 567.9529418945312\n\nEpoch Loss pair 499 567.5263061523438\n\nEpoch Loss pair 499 567.1002197265625\n\nEpoch Loss pair 499 566.6744995117188\n\nEpoch Loss pair 499 566.2492065429688\n\nEpoch Loss pair 499 565.8244018554688\n\nEpoch Loss pair 499 565.400146484375\n\nEpoch Loss pair 499 564.9771728515625\n\nEpoch Loss pair 499 564.5556640625\n\nEpoch Loss pair 499 564.1343994140625\n\nEpoch Loss pair 499 563.7137451171875\n\nEpoch Loss pair 499 563.2933959960938\n\nEpoch Loss pair 499 562.87353515625\n\nEpoch Loss pair 499 562.4545288085938\n\nEpoch Loss pair 499 562.0361938476562\n\nEpoch Loss pair 499 561.6183471679688\n\nEpoch Loss pair 499 561.2008056640625\n\nEpoch Loss pair 499 560.7843017578125\n\nEpoch Loss pair 499 560.3690185546875\n\nEpoch Loss pair 499 559.9544677734375\n\nEpoch Loss pair 499 559.5404663085938\n\nEpoch Loss pair 499 559.1265869140625\n\nEpoch Loss pair 499 558.7133178710938\n\nEpoch Loss pair 499 558.300537109375\n\nEpoch Loss pair 499 557.8884887695312\n\nEpoch Loss pair 499 557.4771118164062\n\nEpoch Loss pair 499 557.0662841796875\n\nEpoch Loss pair 499 556.6561279296875\n\nEpoch Loss pair 499 556.24658203125\n\nEpoch Loss pair 499 555.8378295898438\n\nEpoch Loss pair 499 555.429931640625\n\nEpoch Loss pair 499 555.0224609375\n\nEpoch Loss pair 499 554.6153564453125\n\nEpoch Loss pair 499 554.2088623046875\n\nEpoch Loss pair 499 553.8029174804688\n\nEpoch Loss pair 499 553.3977661132812\n\nEpoch Loss pair 499 552.9932250976562\n\nEpoch Loss pair 499 552.5894775390625\n\nEpoch Loss pair 499 552.18603515625\n\nEpoch Loss pair 499 551.7828979492188\n\nEpoch Loss pair 499 551.3802490234375\n\nEpoch Loss pair 499 550.9781494140625\n\nEpoch Loss pair 499 550.576416015625\n\nEpoch Loss pair 499 550.17529296875\n\nEpoch Loss pair 499 549.7748413085938\n\nEpoch Loss pair 499549.3749389648438\n\nEpoch Loss pair 499 548.9752197265625\n\nEpoch Loss pair 499 548.5761108398438\n\nEpoch Loss pair 499 548.1773071289062\n\nEpoch Loss pair 499 547.7789306640625\n\nEpoch Loss pair 499 547.3809204101562\n\nEpoch Loss pair 499 546.9833374023438\n\nEpoch Loss pair 499 546.586181640625\n\nEpoch Loss pair 499 546.1893920898438\n\nEpoch Loss pair 499 545.7931518554688\n\nEpoch Loss pair 499 545.3973388671875\n\nEpoch Loss pair 499 545.0018310546875\n\nEpoch Loss pair 499 544.6068725585938\n\nEpoch Loss pair 499 544.2122192382812\n\nEpoch Loss pair 499 543.8179931640625\n\nEpoch Loss pair 499 543.4243774414062\n\nEpoch Loss pair 499 543.0314331054688\n\nEpoch Loss pair 499 542.6388549804688\n\nEpoch Loss pair 499 542.2467041015625\n\nEpoch Loss pair 499 541.85546875\n\nEpoch Loss pair 499 541.46484375\n\nEpoch Loss pair 499 541.0745849609375\n\nEpoch Loss pair 499 540.6845703125\n\nEpoch Loss pair 499 540.2950439453125\n\nEpoch Loss pair 499 539.906005859375\n\nEpoch Loss pair 499 539.5171508789062\n\nEpoch Loss pair 499 539.1287841796875\n\nEpoch Loss pair 499 538.7409057617188\n\nEpoch Loss pair 499 538.353271484375\n\nEpoch Loss pair 499 537.9661865234375\n\nEpoch Loss pair 499 537.579345703125\n\nEpoch Loss pair 499 537.1932373046875\n\nEpoch Loss pair 499 536.8075561523438\n\nEpoch Loss pair 499 536.4223022460938\n\nEpoch Loss pair 499 536.0377197265625\n\nEpoch Loss pair 499 535.6533813476562\n\nEpoch Loss pair 499 535.2694091796875\n\nEpoch Loss pair 499 534.8857421875\n\nEpoch Loss pair 499 534.5032348632812\n\nEpoch Loss pair 499 534.1212768554688\n\nEpoch Loss pair 499 533.7398071289062\n\nEpoch Loss pair 499 533.3587036132812\n\nEpoch Loss pair 499 532.9781494140625\n\nEpoch Loss pair 499 532.5979614257812\n\nEpoch Loss pair 499 532.2180786132812\n\nEpoch Loss pair 499 531.8386840820312\n\nEpoch Loss pair 499 531.459716796875\n\nEpoch Loss pair 499 531.0812377929688\n\nEpoch Loss pair 499 530.7033081054688\n\nEpoch Loss pair 499 530.3258056640625\n\nEpoch Loss pair 499 529.948974609375\n\nEpoch Loss pair 499 529.5724487304688\n\nEpoch Loss pair 499 529.1962890625\n\nEpoch Loss pair 499 528.8206176757812\n\nEpoch Loss pair 499 528.446533203125\n\nEpoch Loss pair 499 528.07275390625\n\nEpoch Loss pair 499 527.6992797851562\n\nEpoch Loss pair 499 527.3262329101562\n\nEpoch Loss pair 499 526.95361328125\n\nEpoch Loss pair 499 526.581298828125\n\nEpoch Loss pair 499 526.2092895507812\n\nEpoch Loss pair 499 525.837646484375\n\nEpoch Loss pair 499 525.4663696289062\n\nEpoch Loss pair 499 525.0955200195312\n\nEpoch Loss pair 499 524.7255859375\n\nEpoch Loss pair 499 524.3561401367188\n\nEpoch Loss pair 499 523.9871215820312\n\nEpoch Loss pair 499 523.6182861328125\n\nEpoch Loss pair 499 523.2498168945312\n\nEpoch Loss pair 499 522.8817138671875\n\nEpoch Loss pair 499 522.513916015625\n\nEpoch Loss pair 499 522.1471557617188\n\nEpoch Loss pair 499 521.7811279296875\n\nEpoch Loss pair 499 521.4153442382812\n\nEpoch Loss pair 499 521.0506591796875\n\nEpoch Loss pair 499 520.6864013671875\n\nEpoch Loss pair 499 520.32275390625\n\nEpoch Loss pair 499 519.9593505859375\n\nEpoch Loss pair 499 519.5963134765625\n\nEpoch Loss pair 499 519.23388671875\n\nEpoch Loss pair 499 518.8716430664062\n\nEpoch Loss pair 499 518.5098876953125\n\nEpoch Loss pair 499 518.1484375\n\nEpoch Loss pair 499 517.787353515625\n\nEpoch Loss pair 499 517.4265747070312\n\nEpoch Loss pair 499 517.0662841796875\n\nEpoch Loss pair 499 516.7064819335938\n\nEpoch Loss pair 499 516.3469848632812\n\nEpoch Loss pair 499 515.9879150390625\n\nEpoch Loss pair 499 515.6295776367188\n\nEpoch Loss pair 499 515.271484375\n\nEpoch Loss pair 499 514.9140014648438\n\nEpoch Loss pair 499 514.5567626953125\n\nEpoch Loss pair 499 514.2000122070312\n\nEpoch Loss pair 499 513.84375\n\nEpoch Loss pair 499 513.4879150390625\n\nEpoch Loss pair 499 513.1328125\n\nEpoch Loss pair 499 512.7781372070312\n\nEpoch Loss pair 499 512.4236450195312\n\nEpoch Loss pair 499 512.0694580078125\n\nEpoch Loss pair 499 511.7156982421875\n\nEpoch Loss pair 499 511.36248779296875\n\nEpoch Loss pair 499 511.010009765625\n\nEpoch Loss pair 499 510.65802001953125\n\nEpoch Loss pair 499 510.30645751953125\n\nEpoch Loss pair 499 509.95538330078125\n\nEpoch Loss pair 499 509.6045837402344\n\nEpoch Loss pair 499 509.2546691894531\n\nEpoch Loss pair 499 508.9057312011719\n\nEpoch Loss pair 499 508.5574951171875\n\nEpoch Loss pair 499 508.20977783203125\n\nEpoch Loss pair 499 507.8625183105469\n\nEpoch Loss pair 499 507.5156555175781\n\nEpoch Loss pair 499 507.16937255859375\n\nEpoch Loss pair 499 506.8232421875\n\nEpoch Loss pair 499 506.47747802734375\n\nEpoch Loss pair 499 506.1319580078125\n\nEpoch Loss pair 499 505.7870178222656\n\nEpoch Loss pair 499 505.4423828125\n\nEpoch Loss pair 499 505.0980224609375\n\nEpoch Loss pair 499 504.754150390625\n\nEpoch Loss pair 499 504.41082763671875\n\nEpoch Loss pair 499 504.0678405761719\n\nEpoch Loss pair 499 503.72503662109375\n\nEpoch Loss pair 499 503.3824157714844\n\nEpoch Loss pair 499 503.040283203125\n\nEpoch Loss pair 499 502.6988525390625\n\nEpoch Loss pair 499 502.357666015625\n\nEpoch Loss pair 499 502.01690673828125\n\nEpoch Loss pair 499 501.67633056640625\n\nEpoch Loss pair 499 501.3361511230469\n\nEpoch Loss pair 499 500.9960632324219\n\nEpoch Loss pair 499 500.6564025878906\n\nEpoch Loss pair 499 500.3171081542969\n\nEpoch Loss pair 499 499.9779052734375\n\nEpoch Loss pair 499 499.6392517089844\n\nEpoch Loss pair 499 499.3010559082031\n\nEpoch Loss pair 499 498.9631042480469\n\nEpoch Loss pair 499 498.62542724609375\n\nEpoch Loss pair 499 498.28802490234375\n\nEpoch Loss pair 499 497.9508361816406\n\nEpoch Loss pair 499 497.61407470703125\n\nEpoch Loss pair 499 497.2776184082031\n\nEpoch Loss pair 499 496.94146728515625\n\nEpoch Loss pair 499 496.60552978515625\n\nEpoch Loss pair 499 496.2705383300781\n\nEpoch Loss pair 499 495.93609619140625\n\nEpoch Loss pair 499 495.602294921875\n\nEpoch Loss pair 499 495.268798828125\n\nEpoch Loss pair 499 494.93572998046875\n\nEpoch Loss pair 499 494.6030578613281\n\nEpoch Loss pair 499 494.2710266113281\n\nEpoch Loss pair 499 493.93963623046875\n\nEpoch Loss pair 499 493.6087646484375\n\nEpoch Loss pair 499 493.2781982421875\n\nEpoch Loss pair 499 492.94781494140625\n\nEpoch Loss pair 499 492.61785888671875\n\nEpoch Loss pair 499 492.2882995605469\n\nEpoch Loss pair 499 491.9593505859375\n\nEpoch Loss pair 499 491.63055419921875\n\nEpoch Loss pair 499 491.302001953125\n\nEpoch Loss pair 499 490.97369384765625\n\nEpoch Loss pair 499 490.6458435058594\n\nEpoch Loss pair 499 490.3188781738281\n\nEpoch Loss pair 499 489.9920654296875\n\nEpoch Loss pair 499 489.6659240722656\n\nEpoch Loss pair 499 489.34014892578125\n\nEpoch Loss pair 499 489.01483154296875\n\nEpoch Loss pair 499 488.68988037109375\n\nEpoch Loss pair 499 488.3655090332031\n\nEpoch Loss pair 499 488.0413818359375\n\nEpoch Loss pair 499 487.7174987792969\n\nEpoch Loss pair 499 487.3939514160156\n\nEpoch Loss pair 499 487.0713806152344\n\nEpoch Loss pair 499 486.7490234375\n\nEpoch Loss pair 499 486.42706298828125\n\nEpoch Loss pair 499 486.10528564453125\n\nEpoch Loss pair 499 485.7838134765625\n\nEpoch Loss pair 499 485.46258544921875\n\nEpoch Loss pair 499 485.1416320800781\n\nEpoch Loss pair 499 484.8209228515625\n\nEpoch Loss pair 499 484.50042724609375\n\nEpoch Loss pair 499 484.18023681640625\n\nEpoch Loss pair 499 483.8603210449219\n\nEpoch Loss pair 499 483.5409240722656\n\nEpoch Loss pair 499 483.2223815917969\n\nEpoch Loss pair 499 482.9042053222656\n\nEpoch Loss pair 499 482.58648681640625\n\nEpoch Loss pair 499 482.2691955566406\n\nEpoch Loss pair 499 481.9521179199219\n\nEpoch Loss pair 499 481.63531494140625\n\nEpoch Loss pair 499 481.3188171386719\n\nEpoch Loss pair 499 481.0024719238281\n\nEpoch Loss pair 499 480.68646240234375\n\nEpoch Loss pair 499 480.3707275390625\n\nEpoch Loss pair 499 480.05511474609375\n\nEpoch Loss pair 499 479.7398376464844\n\nEpoch Loss pair 499 479.4248046875\n\nEpoch Loss pair 499 479.1100769042969\n\nEpoch Loss pair 499 478.79571533203125\n\nEpoch Loss pair 499 478.481689453125\n\nEpoch Loss pair 499 478.16790771484375\n\nEpoch Loss pair 499 477.8545837402344\n\nEpoch Loss pair 499 477.54144287109375\n\nEpoch Loss pair 499 477.2288818359375\n\nEpoch Loss pair 499 476.916259765625\n\nEpoch Loss pair 499 476.60406494140625\n\nEpoch Loss pair 499 476.29241943359375\n\nEpoch Loss pair 499 475.98095703125\n\nEpoch Loss pair 499 475.66973876953125\n\nEpoch Loss pair 499 475.35858154296875\n\nEpoch Loss pair 499 475.0478515625\n\nEpoch Loss pair 499 474.7372741699219\n\nEpoch Loss pair 499 474.42694091796875\n\nEpoch Loss pair 499 474.1171875\n\nEpoch Loss pair 499 473.8076477050781\n\nEpoch Loss pair 499 473.4983215332031\n\nEpoch Loss pair 499 473.1892395019531\n\nEpoch Loss pair 499 472.88037109375\n\nEpoch Loss pair 499 472.5718688964844\n\nEpoch Loss pair 499 472.263916015625\n\nEpoch Loss pair499 471.95611572265625\n\nEpoch Loss pair 499 471.64886474609375\n\nEpoch Loss pair 499 471.34197998046875\n\nEpoch Loss pair 499 471.0352478027344\n\nEpoch Loss pair 499 470.7289123535156\n\nEpoch Loss pair 499 470.4227600097656\n\nEpoch Loss pair 499 470.1167297363281\n\nEpoch Loss pair 499 469.8112487792969\n\nEpoch Loss pair 499 469.5062561035156\n\nEpoch Loss pair 499 469.20147705078125\n\nEpoch Loss pair 499 468.897216796875\n\nEpoch Loss pair 499 468.5933837890625\n\nEpoch Loss pair 499 468.2899169921875\n\nEpoch Loss pair 499 467.9867248535156\n\nEpoch Loss pair 499 467.6838073730469\n\nEpoch Loss pair 499 467.3810729980469\n\nEpoch Loss pair 499 467.0786437988281\n\nEpoch Loss pair 499 466.7764892578125\n\nEpoch Loss pair 499 466.47442626953125\n\nEpoch Loss pair 499 466.17266845703125\n\nEpoch Loss pair 499 465.87109375\n\nEpoch Loss pair 499 465.5697021484375\n\nEpoch Loss pair 499 465.26861572265625\n\nEpoch Loss pair 499 464.9678039550781\n\nEpoch Loss pair 499 464.667236328125\n\nEpoch Loss pair 499 464.3669128417969\n\nEpoch Loss pair 499 464.0667419433594\n\nEpoch Loss pair 499 463.7667236328125\n\nEpoch Loss pair 499 463.4671936035156\n\nEpoch Loss pair 499 463.1680603027344\n\nEpoch Loss pair 499 462.869140625\n\nEpoch Loss pair 499 462.5704345703125\n\nEpoch Loss pair 499 462.2720947265625\n\nEpoch Loss pair 499 461.9742431640625\n\nEpoch Loss pair 499 461.67681884765625\n\nEpoch Loss pair 499 461.3797302246094\n\nEpoch Loss pair 499 461.082763671875\n\nEpoch Loss pair 499 460.78607177734375\n\nEpoch Loss pair 499 460.4896240234375\n\nEpoch Loss pair 499 460.19329833984375\n\nEpoch Loss pair 499 459.89727783203125\n\nEpoch Loss pair 499 459.6015319824219\n\nEpoch Loss pair 499 459.3059387207031\n\nEpoch Loss pair 499 459.0105895996094\n\nEpoch Loss pair 499 458.7158203125\n\nEpoch Loss pair 499 458.42138671875\n\nEpoch Loss pair 499 458.12762451171875\n\nEpoch Loss pair 499 457.83428955078125\n\nEpoch Loss pair 499 457.54119873046875\n\nEpoch Loss pair 499 457.2486267089844\n\nEpoch Loss pair 499 456.9564514160156\n\nEpoch Loss pair 499 456.66424560546875\n\nEpoch Loss pair 499 456.3724365234375\n\nEpoch Loss pair 499 456.0810241699219\n\nEpoch Loss pair 499 455.78985595703125\n\nEpoch Loss pair 499 455.4988708496094\n\nEpoch Loss pair 499 455.20806884765625\n\nEpoch Loss pair 499 454.91766357421875\n\nEpoch Loss pair 499 454.6275329589844\n\nEpoch Loss pair 499 454.3376159667969\n\nEpoch Loss pair 499 454.0480041503906\n\nEpoch Loss pair 499 453.7587890625\n\nEpoch Loss pair 499 453.47021484375\n\nEpoch Loss pair 499 453.1817626953125\n\nEpoch Loss pair 499 452.89373779296875\n\nEpoch Loss pair 499 452.6061096191406\n\nEpoch Loss pair 499 452.31866455078125\n\nEpoch Loss pair 499 452.03131103515625\n\nEpoch Loss pair 499 451.7442932128906\n\nEpoch Loss pair 499 451.4573059082031\n\nEpoch Loss pair 499 451.17059326171875\n\nEpoch Loss pair 499 450.8841857910156\n\nEpoch Loss pair 499 450.5980224609375\n\nEpoch Loss pair 499 450.31201171875\n\nEpoch Loss pair 499 450.0263366699219\n\nEpoch Loss pair 499 449.7408142089844\n\nEpoch Loss pair 499 449.4556884765625\n\nEpoch Loss pair 499 449.1708068847656\n\nEpoch Loss pair 499 448.88616943359375\n\nEpoch Loss pair 499 448.6016540527344\n\nEpoch Loss pair 499 448.3173522949219\n\nEpoch Loss pair 499 448.03338623046875\n\nEpoch Loss pair 499 447.74951171875\n\nEpoch Loss pair 499 447.46575927734375\n\nEpoch Loss pair 499 447.1821594238281\n\nEpoch Loss pair 499 446.8988952636719\n\nEpoch Loss pair 499 446.6162109375\n\nEpoch Loss pair 499 446.3337707519531\n\nEpoch Loss pair 499 446.05194091796875\n\nEpoch Loss pair 499 445.7706604003906\n\nEpoch Loss pair 499 445.489501953125\n\nEpoch Loss pair 499 445.20849609375\n\nEpoch Loss pair 499 444.9278564453125\n\nEpoch Loss pair 499 444.6475524902344\n\nEpoch Loss pair 499 444.36737060546875\n\nEpoch Loss pair 499 444.08758544921875\n\nEpoch Loss pair 499 443.80780029296875\n\nEpoch Loss pair 499 443.5283203125\n\nEpoch Loss pair 499 443.2489929199219\n\nEpoch Loss pair 499 442.969970703125\n\nEpoch Loss pair 499 442.691162109375\n\nEpoch Loss pair 499 442.4124755859375\n\nEpoch Loss pair 499 442.1341247558594\n\nEpoch Loss pair 499 441.8562316894531\n\nEpoch Loss pair 499 441.5784912109375\n\nEpoch Loss pair 499 441.3008728027344\n\nEpoch Loss pair 499 441.0234680175781\n\nEpoch Loss pair 499 440.74615478515625\n\nEpoch Loss pair 499 440.4690856933594\n\nEpoch Loss pair 499 440.1921691894531\n\nEpoch Loss pair 499 439.9156188964844\n\nEpoch Loss pair 499 439.63946533203125\n\nEpoch Loss pair 499 439.3641357421875\n\nEpoch Loss pair 499 439.0887451171875\n\nEpoch Loss pair 499 438.8135681152344\n\nEpoch Loss pair 499 438.53863525390625\n\nEpoch Loss pair 499 438.26397705078125\n\nEpoch Loss pair 499 437.9896240234375\n\nEpoch Loss pair 499 437.7148742675781\n\nEpoch Loss pair 499 437.44049072265625\n\nEpoch Loss pair 499 437.16619873046875\n\nEpoch Loss pair 499 436.8919982910156\n\nEpoch Loss pair 499 436.6181945800781\n\nEpoch Loss pair 499 436.34466552734375\n\nEpoch Loss pair 499 436.07122802734375\n\nEpoch Loss pair 499 435.7984924316406\n\nEpoch Loss pair 499 435.5271911621094\n\nEpoch Loss pair 499 435.2562561035156\n\nEpoch Loss pair 499 434.98553466796875\n\nEpoch Loss pair 499 434.7149963378906\n\nEpoch Loss pair 499 434.4446105957031\n\nEpoch Loss pair 499 434.1745910644531\n\nEpoch Loss pair 499 433.90496826171875\n\nEpoch Loss pair 499 433.6354675292969\n\nEpoch Loss pair 499 433.3663330078125\n\nEpoch Loss pair 499 433.0972900390625\n\nEpoch Loss pair 499 432.8284912109375\n\nEpoch Loss pair 499 432.5599060058594\n\nEpoch Loss pair 499 432.2913513183594\n\nEpoch Loss pair 499 432.0231628417969\n\nEpoch Loss pair 499 431.7551574707031\n\nEpoch Loss pair 499 431.4873352050781\n\nEpoch Loss pair 499 431.2196960449219\n\nEpoch Loss pair 499 430.9521789550781\n\nEpoch Loss pair 499 430.6849060058594\n\nEpoch Loss pair 499 430.417724609375\n\nEpoch Loss pair 499 430.1504211425781\n\nEpoch Loss pair 499 429.88330078125\n\nEpoch Loss pair 499 429.61627197265625\n\nEpoch Loss pair 499 429.3494873046875\n\nEpoch Loss pair 499 429.0828857421875\n\nEpoch Loss pair 499 428.8167419433594\n\nEpoch Loss pair 499 428.55078125\n\nEpoch Loss pair 499 428.284912109375\n\nEpoch Loss pair 499 428.0192565917969\n\nEpoch Loss pair 499 427.7536926269531\n\nEpoch Loss pair 499 427.4884033203125\n\nEpoch Loss pair 499 427.2232971191406\n\nEpoch Loss pair 499 426.95843505859375\n\nEpoch Loss pair 499 426.6937561035156\n\nEpoch Loss pair 499 426.4294128417969\n\nEpoch Loss pair 499 426.1651611328125\n\nEpoch Loss pair 499 425.90130615234375\n\nEpoch Loss pair 499 425.63763427734375\n\nEpoch Loss pair 499 425.37408447265625\n\nEpoch Loss pair 499 425.11065673828125\n\nEpoch Loss pair 499 424.84759521484375\n\nEpoch Loss pair 499 424.5846252441406\n\nEpoch Loss pair 499 424.3221740722656\n\nEpoch Loss pair 499 424.05987548828125\n\nEpoch Loss pair 499 423.797607421875\n\nEpoch Loss pair 499 423.53558349609375\n\nEpoch Loss pair 499 423.2738037109375\n\nEpoch Loss pair 499 423.0120849609375\n\nEpoch Loss pair 499 422.75128173828125\n\nEpoch Loss pair 499 422.4906921386719\n\nEpoch Loss pair 499 422.2303161621094\n\nEpoch Loss pair 499 421.97003173828125\n\nEpoch Loss pair 499 421.7100830078125\n\n"
    }
   ],
   "source": [
    "# Implement Network with Tensors + AutoGrad + nn Module\n",
    "d_type = torch.float\n",
    "gpu_device = torch.device(\"cuda:0\")\n",
    "\n",
    "# variables\n",
    "batch_size, input_dimensions, hidden_dimensions, output_dimensions = 64, 1000, 100, 10\n",
    "learning_rate = 1e-6\n",
    "# Create random input and output data\n",
    "random_input = torch.randn(batch_size, input_dimensions, device=gpu_device, dtype=d_type)\n",
    "random_output = torch.randn(batch_size, output_dimensions, device=gpu_device, dtype=d_type)\n",
    "\n",
    "# Use the nn package to define our model as a sequence of layers. nn.Sequential\n",
    "# is a Module which contains other Modules, and applies them in sequence to\n",
    "# produce its output. Each Linear Module computes output from input using a\n",
    "# linear function, and holds internal Tensors for its weight and bias.\n",
    "model = nn.Sequential(nn.Linear(in_features=input_dimensions, out_features=hidden_dimensions), nn.ReLU(), nn.Linear(in_features=hidden_dimensions, out_features=output_dimensions),).to(gpu_device)\n",
    "\n",
    "loss_fxn = nn.MSELoss(reduction='sum')\n",
    "\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y by passing x to the model. Module objects\n",
    "    # override the __call__ operator so you can call them like functions. When\n",
    "    # doing so you pass a Tensor of input data to the Module and it produces\n",
    "    # a Tensor of output data.\n",
    "    y_pred = model(random_input)\n",
    "\n",
    "    # Compute and print loss. We pass Tensors containing the predicted and true\n",
    "    # values of y, and the loss function returns a Tensor containing the\n",
    "    # loss.\n",
    "    loss = loss_fxn(y_pred, random_output)\n",
    "    if idx % 100 == 99:\n",
    "        print(\"Epoch Loss pair\", idx, loss.item())\n",
    "        print()\n",
    "\n",
    "    # Zero the gradients before running the backward pass.\n",
    "    model.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to all the learnable\n",
    "    # parameters of the model. Internally, the parameters of each Module are stored\n",
    "    # in Tensors with requires_grad=True, so this call will compute gradients for\n",
    "    # all learnable parameters in the model.\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the weights using gradient descent. Each parameter is a Tensor, so\n",
    "    # we can access its gradients like we did before.\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate * param.grad\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "och Loss pair 499 501.0843505859375\n\nEpoch Loss pair 499 489.2422790527344\n\nEpoch Loss pair 499 477.72760009765625\n\nEpoch Loss pair 499 466.57550048828125\n\nEpoch Loss pair 499 455.7320251464844\n\nEpoch Loss pair 499 445.2145690917969\n\nEpoch Loss pair 499 435.00799560546875\n\nEpoch Loss pair 499 425.0719299316406\n\nEpoch Loss pair 499 415.3626403808594\n\nEpoch Loss pair 499 405.9352111816406\n\nEpoch Loss pair 499 396.7318115234375\n\nEpoch Loss pair 499 387.76458740234375\n\nEpoch Loss pair 499 378.9987487792969\n\nEpoch Loss pair 499 370.4552001953125\n\nEpoch Loss pair 499 362.1180419921875\n\nEpoch Loss pair 499 353.963623046875\n\nEpoch Loss pair 499 346.0450439453125\n\nEpoch Loss pair 499 338.3359680175781\n\nEpoch Loss pair 499 330.781005859375\n\nEpoch Loss pair 499 323.3877258300781\n\nEpoch Loss pair 499 316.14581298828125\n\nEpoch Loss pair 499 309.04388427734375\n\nEpoch Loss pair 499 302.0611267089844\n\nEpoch Loss pair 499 295.2125549316406\n\nEpoch Loss pair 499 288.48486328125\n\nEpoch Loss pair 499 281.8809509277344\n\nEpoch Loss pair 499 275.4063415527344\n\nEpoch Loss pair 499 269.05859375\n\nEpoch Loss pair 499 262.82342529296875\n\nEpoch Loss pair 499 256.7354431152344\n\nEpoch Loss pair 499 250.77005004882812\n\nEpoch Loss pair 499 244.90475463867188\n\nEpoch Loss pair 499 239.1507568359375\n\nEpoch Loss pair 499 233.51353454589844\n\nEpoch Loss pair 499 227.9790496826172\n\nEpoch Loss pair 499 222.54347229003906\n\nEpoch Loss pair 499 217.22293090820312\n\nEpoch Loss pair 499 211.99295043945312\n\nEpoch Loss pair 499 206.86944580078125\n\nEpoch Loss pair 499 201.86187744140625\n\nEpoch Loss pair 499 196.9546661376953\n\nEpoch Loss pair 499 192.1343536376953\n\nEpoch Loss pair 499 187.39935302734375\n\nEpoch Loss pair 499 182.76548767089844\n\nEpoch Loss pair 499 178.22731018066406\n\nEpoch Loss pair 499 173.78607177734375\n\nEpoch Loss pair 499 169.44036865234375\n\nEpoch Loss pair 499 165.16395568847656\n\nEpoch Loss pair 499 160.97158813476562\n\nEpoch Loss pair 499 156.87002563476562\n\nEpoch Loss pair 499 152.8479461669922\n\nEpoch Loss pair 499 148.8988800048828\n\nEpoch Loss pair 499 145.02845764160156\n\nEpoch Loss pair 499 141.24085998535156\n\nEpoch Loss pair 499 137.53309631347656\n\nEpoch Loss pair 499 133.9024200439453\n\nEpoch Loss pair 499 130.34788513183594\n\nEpoch Loss pair 499 126.85844421386719\n\nEpoch Loss pair 499 123.44867706298828\n\nEpoch Loss pair 499 120.09342956542969\n\nEpoch Loss pair 499 116.81246185302734\n\nEpoch Loss pair 499 113.59964752197266\n\nEpoch Loss pair 499 110.45000457763672\n\nEpoch Loss pair 499 107.36465454101562\n\nEpoch Loss pair 499 104.35265350341797\n\nEpoch Loss pair 499 101.4061050415039\n\nEpoch Loss pair 499 98.51973724365234\n\nEpoch Loss pair 499 95.69400024414062\n\nEpoch Loss pair 499 92.9322738647461\n\nEpoch Loss pair 499 90.23286437988281\n\nEpoch Loss pair 499 87.5906753540039\n\nEpoch Loss pair 499 85.0108642578125\n\nEpoch Loss pair 499 82.4880599975586\n\nEpoch Loss pair 499 80.0252456665039\n\nEpoch Loss pair 499 77.61508178710938\n\nEpoch Loss pair 499 75.26115417480469\n\nEpoch Loss pair 499 72.9659423828125\n\nEpoch Loss pair 499 70.7211685180664\n\nEpoch Loss pair 499 68.52547454833984\n\nEpoch Loss pair 499 66.38558197021484\n\nEpoch Loss pair 499 64.2951431274414\n\nEpoch Loss pair 499 62.25217056274414\n\nEpoch Loss pair 499 60.26277542114258\n\nEpoch Loss pair 499 58.319698333740234\n\nEpoch Loss pair 499 56.427818298339844\n\nEpoch Loss pair 499 54.582889556884766\n\nEpoch Loss pair 499 52.78831481933594\n\nEpoch Loss pair 499 51.04042053222656\n\nEpoch Loss pair 499 49.33872604370117\n\nEpoch Loss pair 499 47.68098831176758\n\nEpoch Loss pair 499 46.068939208984375\n\nEpoch Loss pair 499 44.49907684326172\n\nEpoch Loss pair 499 42.970863342285156\n\nEpoch Loss pair 499 41.48339080810547\n\nEpoch Loss pair 499 40.03564453125\n\nEpoch Loss pair 499 38.629234313964844\n\nEpoch Loss pair 499 37.2595329284668\n\nEpoch Loss pair 499 35.930702209472656\n\nEpoch Loss pair 499 34.63984680175781\n\nEpoch Loss pair 499 33.38700485229492\n\nEpoch Loss pair 499 32.168460845947266\n\nEpoch Loss pair 499 30.98619842529297\n\nEpoch Loss pair 499 29.841482162475586\n\nEpoch Loss pair 499 28.729480743408203\n\nEpoch Loss pair 499 27.6512451171875\n\nEpoch Loss pair 499 26.606769561767578\n\nEpoch Loss pair 499 25.596155166625977\n\nEpoch Loss pair 499 24.617887496948242\n\nEpoch Loss pair 499 23.671348571777344\n\nEpoch Loss pair 499 22.757923126220703\n\nEpoch Loss pair 499 21.873029708862305\n\nEpoch Loss pair 499 21.01765251159668\n\nEpoch Loss pair 499 20.1919002532959\n\nEpoch Loss pair 499 19.393129348754883\n\nEpoch Loss pair 499 18.620948791503906\n\nEpoch Loss pair 499 17.87512969970703\n\nEpoch Loss pair 499 17.155683517456055\n\nEpoch Loss pair 499 16.46148681640625\n\nEpoch Loss pair 499 15.792750358581543\n\nEpoch Loss pair 499 15.147892951965332\n\nEpoch Loss pair 499 14.526400566101074\n\nEpoch Loss pair 499 13.926443099975586\n\nEpoch Loss pair 499 13.348970413208008\n\nEpoch Loss pair 499 12.792535781860352\n\nEpoch Loss pair 499 12.256711959838867\n\nEpoch Loss pair 499 11.740612030029297\n\nEpoch Loss pair 499 11.243040084838867\n\nEpoch Loss pair 499 10.764521598815918\n\nEpoch Loss pair 499 10.30373477935791\n\nEpoch Loss pair 499 9.860803604125977\n\nEpoch Loss pair 499 9.434904098510742\n\nEpoch Loss pair 499 9.025842666625977\n\nEpoch Loss pair 499 8.632288932800293\n\nEpoch Loss pair 499 8.254947662353516\n\nEpoch Loss pair 499 7.89280891418457\n\nEpoch Loss pair 499 7.545103549957275\n\nEpoch Loss pair 499 7.211160659790039\n\nEpoch Loss pair 499 6.890621185302734\n\nEpoch Loss pair 499 6.582432746887207\n\nEpoch Loss pair 499 6.287074089050293\n\nEpoch Loss pair 499 6.004399299621582\n\nEpoch Loss pair 499 5.733611106872559\n\nEpoch Loss pair 499 5.473905563354492\n\nEpoch Loss pair 499 5.22469425201416\n\nEpoch Loss pair 499 4.986336708068848\n\nEpoch Loss pair 499 4.758176803588867\n\nEpoch Loss pair 499 4.539506912231445\n\nEpoch Loss pair 499 4.330381870269775\n\nEpoch Loss pair 499 4.1300129890441895\n\nEpoch Loss pair 499 3.9385428428649902\n\nEpoch Loss pair 499 3.7551305294036865\n\nEpoch Loss pair 499 3.5798821449279785\n\nEpoch Loss pair 499 3.4122602939605713\n\nEpoch Loss pair 499 3.2520718574523926\n\nEpoch Loss pair 499 3.098907709121704\n\nEpoch Loss pair 499 2.9528143405914307\n\nEpoch Loss pair 499 2.8130440711975098\n\nEpoch Loss pair 499 2.6797327995300293\n\nEpoch Loss pair 499 2.5522875785827637\n\nEpoch Loss pair 499 2.4307289123535156\n\nEpoch Loss pair 499 2.314849376678467\n\nEpoch Loss pair 499 2.204333782196045\n\nEpoch Loss pair 499 2.098844528198242\n\nEpoch Loss pair 499 1.998243808746338\n\nEpoch Loss pair 499 1.9023211002349854\n\nEpoch Loss pair 499 1.8109568357467651\n\nEpoch Loss pair 499 1.7238472700119019\n\nEpoch Loss pair 499 1.6409496068954468\n\nEpoch Loss pair 499 1.5619840621948242\n\nEpoch Loss pair 499 1.4867992401123047\n\nEpoch Loss pair 499 1.4151362180709839\n\nEpoch Loss pair 499 1.3468513488769531\n\nEpoch Loss pair 499 1.2818516492843628\n\nEpoch Loss pair 499 1.2199221849441528\n\nEpoch Loss pair 499 1.1610381603240967\n\nEpoch Loss pair 499 1.1049461364746094\n\nEpoch Loss pair 499 1.051546573638916\n\nEpoch Loss pair 499 1.0007601976394653\n\nEpoch Loss pair 499 0.9524083733558655\n\nEpoch Loss pair 499 0.9064540266990662\n\nEpoch Loss pair 499 0.8627532124519348\n\nEpoch Loss pair 499 0.821150004863739\n\nEpoch Loss pair 499 0.7817090153694153\n\nEpoch Loss pair 499 0.744217038154602\n\nEpoch Loss pair 499 0.7085456252098083\n\nEpoch Loss pair 499 0.6745877265930176\n\nEpoch Loss pair 499 0.6422980427742004\n\nEpoch Loss pair 499 0.6116055846214294\n\nEpoch Loss pair 499 0.5823878049850464\n\nEpoch Loss pair 499 0.5546275973320007\n\nEpoch Loss pair 499 0.5282089710235596\n\nEpoch Loss pair 499 0.5030936002731323\n\nEpoch Loss pair 499 0.4792182445526123\n\nEpoch Loss pair 499 0.456522136926651\n\nEpoch Loss pair 499 0.4349427819252014\n\nEpoch Loss pair 499 0.4144124388694763\n\nEpoch Loss pair 499 0.3948712646961212\n\nEpoch Loss pair 499 0.3763055205345154\n\nEpoch Loss pair 499 0.3586437404155731\n\nEpoch Loss pair 499 0.3418569564819336\n\nEpoch Loss pair 499 0.32588085532188416\n\nEpoch Loss pair 499 0.31069278717041016\n\nEpoch Loss pair 499 0.2962452471256256\n\nEpoch Loss pair 499 0.2825087010860443\n\nEpoch Loss pair 499 0.26941990852355957\n\nEpoch Loss pair 499 0.2569725811481476\n\nEpoch Loss pair 499 0.2451237589120865\n\nEpoch Loss pair 499 0.23385074734687805\n\nEpoch Loss pair 499 0.22312051057815552\n\nEpoch Loss pair 499 0.21291038393974304\n\nEpoch Loss pair 499 0.20321466028690338\n\nEpoch Loss pair 499 0.19397909939289093\n\nEpoch Loss pair 499 0.18518930673599243\n\nEpoch Loss pair 499 0.17682604491710663\n\nEpoch Loss pair 499 0.1688588410615921\n\nEpoch Loss pair 499 0.16128067672252655\n\nEpoch Loss pair 499 0.15404880046844482\n\nEpoch Loss pair 499 0.14716999232769012\n\nEpoch Loss pair 499 0.14061452448368073\n\nEpoch Loss pair 499 0.13436801731586456\n\nEpoch Loss pair 499 0.1284160017967224\n\nEpoch Loss pair 499 0.12274140864610672\n\nEpoch Loss pair 499 0.11733416467905045\n\nEpoch Loss pair 499 0.1121797189116478\n\nEpoch Loss pair 499 0.10726823657751083\n\nEpoch Loss pair 499 0.10258452594280243\n\nEpoch Loss pair 499 0.09810855984687805\n\nEpoch Loss pair 499 0.0938468724489212\n\nEpoch Loss pair 499 0.08978050202131271\n\nEpoch Loss pair 499 0.0859043151140213\n\nEpoch Loss pair 499 0.08220089226961136\n\nEpoch Loss pair 499 0.0786689966917038\n\nEpoch Loss pair 499 0.07529406249523163\n\nEpoch Loss pair 499 0.0720735490322113\n\nEpoch Loss pair 499 0.06899780035018921\n\nEpoch Loss pair 499 0.06605876237154007\n\nEpoch Loss pair 499 0.06325118243694305\n\nEpoch Loss pair 499 0.06056768447160721\n\nEpoch Loss pair 499 0.05800369009375572\n\nEpoch Loss pair 499 0.05555201694369316\n\nEpoch Loss pair 499 0.0532086007297039\n\nEpoch Loss pair 499 0.0509675107896328\n\nEpoch Loss pair 499 0.04882626235485077\n\nEpoch Loss pair 499 0.046781450510025024\n\nEpoch Loss pair 499 0.04482511058449745\n\nEpoch Loss pair 499 0.042952898889780045\n\nEpoch Loss pair 499 0.04116349667310715\n\nEpoch Loss pair 499 0.0394505076110363\n\nEpoch Loss pair 499 0.03781159222126007\n\nEpoch Loss pair 499 0.036242660135030746\n\nEpoch Loss pair 499 0.034740086644887924\n\nEpoch Loss pair 499 0.0333016961812973\n\nEpoch Loss pair 499 0.03192327544093132\n\nEpoch Loss pair 499 0.0306033156812191\n\nEpoch Loss pair 499 0.029338547959923744\n\nEpoch Loss pair 499 0.0281264316290617\n\nEpoch Loss pair 499 0.02696515992283821\n\nEpoch Loss pair 499 0.025852030143141747\n\nEpoch Loss pair 499 0.024785712361335754\n\nEpoch Loss pair 499 0.023762740194797516\n\nEpoch Loss pair 499 0.022782951593399048\n\nEpoch Loss pair 499 0.021842872723937035\n\nEpoch Loss pair 499 0.020941924303770065\n\nEpoch Loss pair 499 0.020078081637620926\n\nEpoch Loss pair 499 0.019249647855758667\n\nEpoch Loss pair 499 0.018455181270837784\n\nEpoch Loss pair 499 0.01769314520061016\n\nEpoch Loss pair 499 0.01696227304637432\n\nEpoch Loss pair 499 0.01626119390130043\n\nEpoch Loss pair 499 0.01558892335742712\n\nEpoch Loss pair 499 0.014943942427635193\n\nEpoch Loss pair 499 0.014325294643640518\n\nEpoch Loss pair 499 0.013731909915804863\n\nEpoch Loss pair 499 0.013162661343812943\n\nEpoch Loss pair 499 0.012616602703928947\n\nEpoch Loss pair 499 0.012092738412320614\n\nEpoch Loss pair 499 0.011590116657316685\n\nEpoch Loss pair 499 0.011107966303825378\n\nEpoch Loss pair 499 0.010645479895174503\n\nEpoch Loss pair 499 0.010201672092080116\n\nEpoch Loss pair 499 0.009775917045772076\n\nEpoch Loss pair 499 0.009367492981255054\n\nEpoch Loss pair 499 0.008975697681307793\n\nEpoch Loss pair 499 0.008599831722676754\n\nEpoch Loss pair 499 0.008239245042204857\n\nEpoch Loss pair 499 0.007893314585089684\n\nEpoch Loss pair 499 0.0075616235844790936\n\nEpoch Loss pair 499 0.0072433361783623695\n\nEpoch Loss pair 499 0.0069379499182105064\n\nEpoch Loss pair 499 0.006645152345299721\n\nEpoch Loss pair 499 0.006364285014569759\n\nEpoch Loss pair 499 0.006094863172620535\n\nEpoch Loss pair 499 0.0058365752920508385\n\nEpoch Loss pair 499 0.005588727071881294\n\nEpoch Loss pair 499 0.0053511084988713264\n\nEpoch Loss pair 499 0.005123221781104803\n\nEpoch Loss pair 499 0.004904715809971094\n\nEpoch Loss pair 499 0.004695179872214794\n\nEpoch Loss pair 499 0.004494225140661001\n\nEpoch Loss pair 499 0.0043016113340854645\n\nEpoch Loss pair 499 0.004116973374038935\n\nEpoch Loss pair 499 0.003939968068152666\n\nEpoch Loss pair 499 0.0037702075205743313\n\nEpoch Loss pair 499 0.003607550635933876\n\nEpoch Loss pair 499 0.0034516029991209507\n\nEpoch Loss pair 499 0.003302155062556267\n\nEpoch Loss pair 499 0.0031588987912982702\n\nEpoch Loss pair 499 0.0030217021703720093\n\nEpoch Loss pair 499 0.0028900769539177418\n\nEpoch Loss pair 499 0.002764009637758136\n\nEpoch Loss pair 499 0.0026432466693222523\n\nEpoch Loss pair 499 0.0025275188963860273\n\nEpoch Loss pair 499 0.002416666131466627\n\nEpoch Loss pair 499 0.002310474868863821\n\nEpoch Loss pair 499 0.00220871833153069\n\nEpoch Loss pair 499 0.00211128662340343\n\nEpoch Loss pair 499 0.002017964143306017\n\nEpoch Loss pair 499 0.0019285989692434669\n\nEpoch Loss pair 499 0.0018430576892569661\n\nEpoch Loss pair 499 0.0017611191142350435\n\nEpoch Loss pair 499 0.0016826600767672062\n\nEpoch Loss pair 499 0.0016075667226687074\n\nEpoch Loss pair 499 0.001535670948214829\n\nEpoch Loss pair 499 0.0014668578514829278\n\nEpoch Loss pair 499 0.0014010010054334998\n\nEpoch Loss pair 499 0.0013379667652770877\n\nEpoch Loss pair 499 0.0012776522198691964\n\nEpoch Loss pair 499 0.0012199509656056762\n\nEpoch Loss pair 499 0.0011647205101326108\n\nEpoch Loss pair 499 0.001111890422180295\n\nEpoch Loss pair 499 0.0010613594204187393\n\nEpoch Loss pair 499 0.00101302785333246\n\nEpoch Loss pair 499 0.0009668024140410125\n\nEpoch Loss pair 499 0.0009225928806699812\n\nEpoch Loss pair 499 0.0008803240489214659\n\nEpoch Loss pair 499 0.0008398929494433105\n\nEpoch Loss pair 499 0.0008012516191229224\n\nEpoch Loss pair 499 0.0007643284625373781\n\nEpoch Loss pair 499 0.0007290131761692464\n\nEpoch Loss pair 499 0.0006952636176720262\n\nEpoch Loss pair 499 0.0006630101124756038\n\nEpoch Loss pair 499 0.0006321922410279512\n\nEpoch Loss pair 499 0.0006027502240613103\n\nEpoch Loss pair 499 0.0005746062961407006\n\nEpoch Loss pair 499 0.0005477303639054298\n\nEpoch Loss pair 499 0.000522057933267206\n\nEpoch Loss pair 499 0.0004975404590368271\n\nEpoch Loss pair 499 0.0004741257871501148\n\nEpoch Loss pair 499 0.0004517636843957007\n\nEpoch Loss pair 499 0.00043041451135650277\n\nEpoch Loss pair 499 0.000410029519116506\n\nEpoch Loss pair 499 0.0003905685734935105\n\nEpoch Loss pair 499 0.0003719976230058819\n\nEpoch Loss pair 499 0.0003542637568898499\n\nEpoch Loss pair 499 0.00033734863973222673\n\nEpoch Loss pair 499 0.00032120602554641664\n\nEpoch Loss pair 499 0.00030580206657759845\n\nEpoch Loss pair 499 0.0002911076880991459\n\nEpoch Loss pair 499 0.00027708944980986416\n\nEpoch Loss pair 499 0.0002637200814206153\n\nEpoch Loss pair 499 0.0002509703626856208\n\nEpoch Loss pair 499 0.000238806547713466\n\nEpoch Loss pair 499 0.0002272172860102728\n\nEpoch Loss pair 499 0.00021616305457428098\n\nEpoch Loss pair 499 0.00020563279394991696\n\nEpoch Loss pair 499 0.0001956006308319047\n\nEpoch Loss pair 499 0.0001860269985627383\n\nEpoch Loss pair 499 0.00017690355889499187\n\nEpoch Loss pair 499 0.00016821600729599595\n\nEpoch Loss pair 499 0.00015993547276593745\n\nEpoch Loss pair 499 0.00015204830560833216\n\nEpoch Loss pair 499 0.00014452838513534516\n\nEpoch Loss pair 499 0.00013737923291046172\n\nEpoch Loss pair 499 0.00013055703311692923\n\nEpoch Loss pair 499 0.00012406408495735377\n\nEpoch Loss pair 499 0.00011788323172368109\n\nEpoch Loss pair 499 0.00011200340668438002\n\nEpoch Loss pair 499 0.00010639505489962175\n\nEpoch Loss pair 499 0.00010106081026606262\n\nEpoch Loss pair 499 9.598217002348974e-05\n\nEpoch Loss pair 499 9.115447028307244e-05\n\nEpoch Loss pair 499 8.655602869112045e-05\n\nEpoch Loss pair 499 8.218125003622845e-05\n\nEpoch Loss pair 499 7.801753235980868e-05\n\nEpoch Loss pair 499 7.40615141694434e-05\n\nEpoch Loss pair 499 7.029458356555551e-05\n\nEpoch Loss pair 499 6.671091978205368e-05\n\nEpoch Loss pair 499 6.330650649033487e-05\n\nEpoch Loss pair 499 6.006460534990765e-05\n\nEpoch Loss pair 499 5.698669338016771e-05\n\nEpoch Loss pair 499 5.4059488320490345e-05\n\nEpoch Loss pair 499 5.1276972953928635e-05\n\nEpoch Loss pair 499 4.863255162490532e-05\n\nEpoch Loss pair 499 4.6116743760649115e-05\n\nEpoch Loss pair 499 4.372757030068897e-05\n\nEpoch Loss pair 499 4.145853381487541e-05\n\nEpoch Loss pair 499 3.9304348320001736e-05\n\nEpoch Loss pair 499 3.725518399733119e-05\n\nEpoch Loss pair 499 3.5309698432683945e-05\n\nEpoch Loss pair 499 3.3462631108704954e-05\n\nEpoch Loss pair 499 3.170809941366315e-05\n\nEpoch Loss pair 499 3.0041850550333038e-05\n\nEpoch Loss pair 499 2.8460977773647755e-05\n\nEpoch Loss pair 499 2.6960013201460242e-05\n\nEpoch Loss pair 499 2.5534820451866835e-05\n\nEpoch Loss pair 499 2.4182665583794005e-05\n\nEpoch Loss pair 499 2.2898855604580604e-05\n\nEpoch Loss pair 499 2.1681335056200624e-05\n\nEpoch Loss pair 499 2.052675699815154e-05\n\nEpoch Loss pair 499 1.9430615793680772e-05\n\nEpoch Loss pair 499 1.839049582486041e-05\n\nEpoch Loss pair 499 1.740464176691603e-05\n\nEpoch Loss pair 499 1.647019416850526e-05\n\nEpoch Loss pair 499 1.558314215799328e-05\n\nEpoch Loss pair 499 1.4743500287295319e-05\n\nEpoch Loss pair 499 1.3947613297204953e-05\n\nEpoch Loss pair 499 1.3192089681979269e-05\n\nEpoch Loss pair 499 1.2475999028538354e-05\n\nEpoch Loss pair 499 1.1798274499597028e-05\n\nEpoch Loss pair 499 1.115608392865397e-05\n\nEpoch Loss pair 499 1.0546361409069505e-05\n\nEpoch Loss pair 499 9.970757673727348e-06\n\nEpoch Loss pair 499 9.424432391824666e-06\n\nEpoch Loss pair 499 8.905938557290938e-06\n\nEpoch Loss pair 499 8.416463060711976e-06\n\nEpoch Loss pair 499 7.95290179667063e-06\n\nEpoch Loss pair 499 7.512980118917767e-06\n\nEpoch Loss pair 499 7.09798814568785e-06\n\nEpoch Loss pair 499 6.704103270749329e-06\n\nEpoch Loss pair 499 6.331891199806705e-06\n\nEpoch Loss pair 499 5.9790454542962834e-06\n\nEpoch Loss pair 499 5.646126737701707e-06\n\nEpoch Loss pair 499 5.3302314881875645e-06\n\nEpoch Loss pair 499 5.031841737945797e-06\n\nEpoch Loss pair 499 4.749585514218779e-06\n\nEpoch Loss pair 499 4.482515123527264e-06\n\nEpoch Loss pair 499 4.229949354339624e-06\n\nEpoch Loss pair 499 3.990713594248518e-06\n\nEpoch Loss pair 499 3.765271230804501e-06\n\nEpoch Loss pair 499 3.552134558049147e-06\n\nEpoch Loss pair 499 3.350170800331398e-06\n\nEpoch Loss pair 499 3.159486823278712e-06\n\nEpoch Loss pair 499 2.979687224069494e-06\n\nEpoch Loss pair 499 2.809392526614829e-06\n\nEpoch Loss pair 499 2.6492607503314503e-06\n\nEpoch Loss pair 499 2.496814886399079e-06\n\nEpoch Loss pair 499 2.3533793864771724e-06\n\nEpoch Loss pair 499 2.2179726784088416e-06\n\nEpoch Loss pair 499 2.0898160073556937e-06\n\nEpoch Loss pair 499 1.9690764929691795e-06\n\nEpoch Loss pair 499 1.854795641520468e-06\n\nEpoch Loss pair 499 1.7473078059992986e-06\n\nEpoch Loss pair 499 1.64587754625245e-06\n\nEpoch Loss pair 499 1.5499841765631572e-06\n\nEpoch Loss pair 499 1.4594177173421485e-06\n\nEpoch Loss pair 499 1.374482394567167e-06\n\nEpoch Loss pair 499 1.293742457164626e-06\n\nEpoch Loss pair 499 1.2174692756161676e-06\n\nEpoch Loss pair 499 1.1461178246463533e-06\n\nEpoch Loss pair 499 1.0786626489789342e-06\n\nEpoch Loss pair 499 1.0147149396289024e-06\n\nEpoch Loss pair 499 9.547403578835656e-07\n\nEpoch Loss pair 499 8.981603514257586e-07\n\nEpoch Loss pair 499 8.447754566986987e-07\n\nEpoch Loss pair 499 7.946821369841928e-07\n\nEpoch Loss pair 499 7.472201559721725e-07\n\nEpoch Loss pair 499 7.02576926414622e-07\n\nEpoch Loss pair 499 6.606821898458293e-07\n\nEpoch Loss pair 499 6.212057428456319e-07\n\nEpoch Loss pair 499 5.838015795234242e-07\n\nEpoch Loss pair 499 5.484849339154607e-07\n\nEpoch Loss pair 499 5.154906261850556e-07\n\nEpoch Loss pair 499 4.842445378017146e-07\n\nEpoch Loss pair 499 4.550746837139741e-07\n\nEpoch Loss pair 499 4.274403124782111e-07\n\nEpoch Loss pair 499 4.012647991658014e-07\n\nEpoch Loss pair 499 3.7693590115850384e-07\n\nEpoch Loss pair 499 3.539040278610628e-07\n\nEpoch Loss pair 499 3.323598605220468e-07\n\nEpoch Loss pair 499 3.1192232086141303e-07\n\nEpoch Loss pair 499 2.926879005826777e-07\n\nEpoch Loss pair 499 2.7473564045976673e-07\n\nEpoch Loss pair 499 2.579047020390135e-07\n\nEpoch Loss pair 499 2.41904928088843e-07\n\nEpoch Loss pair 499 2.2696836765589978e-07\n\n"
    }
   ],
   "source": [
    "# Implement Network with Tensors + AutoGrad + nn Module + optim\n",
    "d_type = torch.float\n",
    "gpu_device = torch.device(\"cuda:0\")\n",
    "\n",
    "# variables\n",
    "batch_size, input_dimensions, hidden_dimensions, output_dimensions = 64, 1000, 100, 10\n",
    "learning_rate = 1e-4\n",
    "# Create random input and output data\n",
    "random_input = torch.randn(batch_size, input_dimensions, device=gpu_device, dtype=d_type)\n",
    "random_output = torch.randn(batch_size, output_dimensions, device=gpu_device, dtype=d_type)\n",
    "\n",
    "# Use the nn package to define our model as a sequence of layers. nn.Sequential\n",
    "# is a Module which contains other Modules, and applies them in sequence to\n",
    "# produce its output. Each Linear Module computes output from input using a\n",
    "# linear function, and holds internal Tensors for its weight and bias.\n",
    "model = nn.Sequential(nn.Linear(in_features=input_dimensions, out_features=hidden_dimensions), nn.ReLU(), nn.Linear(in_features=hidden_dimensions, out_features=output_dimensions),).to(gpu_device)\n",
    "\n",
    "loss_fxn = nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y by passing x to the model. Module objects\n",
    "    # override the __call__ operator so you can call them like functions. When\n",
    "    # doing so you pass a Tensor of input data to the Module and it produces\n",
    "    # a Tensor of output data.\n",
    "    y_pred = model(random_input)\n",
    "\n",
    "    # Compute and print loss. We pass Tensors containing the predicted and true\n",
    "    # values of y, and the loss function returns a Tensor containing the\n",
    "    # loss.\n",
    "    loss = loss_fxn(y_pred, random_output)\n",
    "    if idx % 100 == 99:\n",
    "        print(\"Epoch Loss pair\", idx, loss.item())\n",
    "        print()\n",
    "\n",
    "    # Zero the gradients before running the backward pass.\n",
    "    model.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to all the learnable\n",
    "    # parameters of the model. Internally, the parameters of each Module are stored\n",
    "    # in Tensors with requires_grad=True, so this call will compute gradients for\n",
    "    # all learnable parameters in the model.\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "499 162.05828857421875\n\nEpoch Loss pair 499 152.57545471191406\n\nEpoch Loss pair 499 143.57264709472656\n\nEpoch Loss pair 499 135.05853271484375\n\nEpoch Loss pair 499 127.01045989990234\n\nEpoch Loss pair 499 119.43370056152344\n\nEpoch Loss pair 499 112.29047393798828\n\nEpoch Loss pair 499 105.57135009765625\n\nEpoch Loss pair 499 99.2163314819336\n\nEpoch Loss pair 499 93.23991394042969\n\nEpoch Loss pair 499 87.62385559082031\n\nEpoch Loss pair 499 82.3311996459961\n\nEpoch Loss pair 499 77.35382843017578\n\nEpoch Loss pair 499 72.6815414428711\n\nEpoch Loss pair 499 68.29139709472656\n\nEpoch Loss pair 499 64.17308044433594\n\nEpoch Loss pair 499 60.30839538574219\n\nEpoch Loss pair 499 56.68817138671875\n\nEpoch Loss pair 499 53.291297912597656\n\nEpoch Loss pair 499 50.074317932128906\n\nEpoch Loss pair 499 47.05964279174805\n\nEpoch Loss pair 499 44.232486724853516\n\nEpoch Loss pair 499 41.57513427734375\n\nEpoch Loss pair 499 39.08771514892578\n\nEpoch Loss pair 499 36.75895309448242\n\nEpoch Loss pair 499 34.57948303222656\n\nEpoch Loss pair 499 32.53874588012695\n\nEpoch Loss pair 499 30.61333465576172\n\nEpoch Loss pair 499 28.8096866607666\n\nEpoch Loss pair 499 27.11776351928711\n\nEpoch Loss pair 499 25.531652450561523\n\nEpoch Loss pair 499 24.044992446899414\n\nEpoch Loss pair 499 22.65073585510254\n\nEpoch Loss pair 499 21.338993072509766\n\nEpoch Loss pair 499 20.107410430908203\n\nEpoch Loss pair 499 18.952388763427734\n\nEpoch Loss pair 499 17.86941146850586\n\nEpoch Loss pair 499 16.85146713256836\n\nEpoch Loss pair 499 15.89659309387207\n\nEpoch Loss pair 499 15.0009126663208\n\nEpoch Loss pair 499 14.160150527954102\n\nEpoch Loss pair 499 13.37189769744873\n\nEpoch Loss pair 499 12.631348609924316\n\nEpoch Loss pair 499 11.934035301208496\n\nEpoch Loss pair 499 11.278636932373047\n\nEpoch Loss pair 499 10.663497924804688\n\nEpoch Loss pair 499 10.084230422973633\n\nEpoch Loss pair 499 9.538329124450684\n\nEpoch Loss pair 499 9.024396896362305\n\nEpoch Loss pair 499 8.540056228637695\n\nEpoch Loss pair 499 8.083710670471191\n\nEpoch Loss pair 499 7.653698921203613\n\nEpoch Loss pair 499 7.24832820892334\n\nEpoch Loss pair 499 6.866192817687988\n\nEpoch Loss pair 499 6.50580358505249\n\nEpoch Loss pair 499 6.165627479553223\n\nEpoch Loss pair 499 5.8447675704956055\n\nEpoch Loss pair 499 5.541626453399658\n\nEpoch Loss pair 499 5.255184650421143\n\nEpoch Loss pair 499 4.984239101409912\n\nEpoch Loss pair 499 4.728381633758545\n\nEpoch Loss pair 499 4.486687660217285\n\nEpoch Loss pair 499 4.25810432434082\n\nEpoch Loss pair 499 4.04235315322876\n\nEpoch Loss pair 499 3.8384015560150146\n\nEpoch Loss pair 499 3.6456120014190674\n\nEpoch Loss pair 499 3.462800979614258\n\nEpoch Loss pair 499 3.289863348007202\n\nEpoch Loss pair 499 3.1261696815490723\n\nEpoch Loss pair 499 2.9711575508117676\n\nEpoch Loss pair 499 2.824322462081909\n\nEpoch Loss pair 499 2.6851930618286133\n\nEpoch Loss pair 499 2.5532758235931396\n\nEpoch Loss pair 499 2.4282305240631104\n\nEpoch Loss pair 499 2.309810161590576\n\nEpoch Loss pair 499 2.1974494457244873\n\nEpoch Loss pair 499 2.0909790992736816\n\nEpoch Loss pair 499 1.9900314807891846\n\nEpoch Loss pair 499 1.8943909406661987\n\nEpoch Loss pair 499 1.803666591644287\n\nEpoch Loss pair 499 1.7176457643508911\n\nEpoch Loss pair 499 1.636108160018921\n\nEpoch Loss pair 499 1.558549404144287\n\nEpoch Loss pair 499 1.4848802089691162\n\nEpoch Loss pair 499 1.4149055480957031\n\nEpoch Loss pair 499 1.3484628200531006\n\nEpoch Loss pair 499 1.2854118347167969\n\nEpoch Loss pair 499 1.2254503965377808\n\nEpoch Loss pair 499 1.1684365272521973\n\nEpoch Loss pair 499 1.1143115758895874\n\nEpoch Loss pair 499 1.0627894401550293\n\nEpoch Loss pair 499 1.0138555765151978\n\nEpoch Loss pair 499 0.9672762751579285\n\nEpoch Loss pair 499 0.9229545593261719\n\nEpoch Loss pair 499 0.8807917237281799\n\nEpoch Loss pair 499 0.8407231569290161\n\nEpoch Loss pair 499 0.8025568127632141\n\nEpoch Loss pair 499 0.7662755250930786\n\nEpoch Loss pair 499 0.7317361235618591\n\nEpoch Loss pair 499 0.6988021731376648\n\nEpoch Loss pair 499 0.6675509810447693\n\nEpoch Loss pair 499 0.6378272771835327\n\nEpoch Loss pair 499 0.6095051169395447\n\nEpoch Loss pair 499 0.5825058221817017\n\nEpoch Loss pair 499 0.5567753314971924\n\nEpoch Loss pair 499 0.5322431325912476\n\nEpoch Loss pair 499 0.5088809132575989\n\nEpoch Loss pair 499 0.4866424798965454\n\nEpoch Loss pair 499 0.4654311239719391\n\nEpoch Loss pair 499 0.44512003660202026\n\nEpoch Loss pair 499 0.42573273181915283\n\nEpoch Loss pair 499 0.4072716236114502\n\nEpoch Loss pair 499 0.3896470069885254\n\nEpoch Loss pair 499 0.3728167414665222\n\nEpoch Loss pair 499 0.35675615072250366\n\nEpoch Loss pair 499 0.3414350152015686\n\nEpoch Loss pair 499 0.3268415331840515\n\nEpoch Loss pair 499 0.3128960132598877\n\nEpoch Loss pair 499 0.2995545268058777\n\nEpoch Loss pair 499 0.286825954914093\n\nEpoch Loss pair 499 0.274666965007782\n\nEpoch Loss pair 499 0.26306387782096863\n\nEpoch Loss pair 499 0.2519805431365967\n\nEpoch Loss pair 499 0.24137412011623383\n\nEpoch Loss pair 499 0.23123909533023834\n\nEpoch Loss pair 499 0.221576988697052\n\nEpoch Loss pair 499 0.2123323678970337\n\nEpoch Loss pair 499 0.20349490642547607\n\nEpoch Loss pair 499 0.19505152106285095\n\nEpoch Loss pair 499 0.18697482347488403\n\nEpoch Loss pair 499 0.17925965785980225\n\nEpoch Loss pair 499 0.17187419533729553\n\nEpoch Loss pair 499 0.1648091971874237\n\nEpoch Loss pair 499 0.15805283188819885\n\nEpoch Loss pair 499 0.1516004353761673\n\nEpoch Loss pair 499 0.14541664719581604\n\nEpoch Loss pair 499 0.13949406147003174\n\nEpoch Loss pair 499 0.13383206725120544\n\nEpoch Loss pair 499 0.12841741740703583\n\nEpoch Loss pair 499 0.12323755025863647\n\nEpoch Loss pair 499 0.11827725917100906\n\nEpoch Loss pair 499 0.11352969706058502\n\nEpoch Loss pair 499 0.1089804396033287\n\nEpoch Loss pair 499 0.1046316996216774\n\nEpoch Loss pair 499 0.10046404600143433\n\nEpoch Loss pair 499 0.09647267311811447\n\nEpoch Loss pair 499 0.0926487147808075\n\nEpoch Loss pair 499 0.08898381888866425\n\nEpoch Loss pair 499 0.08547510951757431\n\nEpoch Loss pair 499 0.08211243152618408\n\nEpoch Loss pair 499 0.07888734340667725\n\nEpoch Loss pair 499 0.07579761743545532\n\nEpoch Loss pair 499 0.07283776253461838\n\nEpoch Loss pair 499 0.07000025361776352\n\nEpoch Loss pair 499 0.06726984679698944\n\nEpoch Loss pair 499 0.06465210020542145\n\nEpoch Loss pair 499 0.06214043125510216\n\nEpoch Loss pair 499 0.059732161462306976\n\nEpoch Loss pair 499 0.057423125952482224\n\nEpoch Loss pair 499 0.055210672318935394\n\nEpoch Loss pair 499 0.053084760904312134\n\nEpoch Loss pair 499 0.05104667693376541\n\nEpoch Loss pair 499 0.04909030348062515\n\nEpoch Loss pair 499 0.04721163958311081\n\nEpoch Loss pair 499 0.0454094223678112\n\nEpoch Loss pair 499 0.04368070513010025\n\nEpoch Loss pair 499 0.042023319751024246\n\nEpoch Loss pair 499 0.04043016955256462\n\nEpoch Loss pair 499 0.03890105336904526\n\nEpoch Loss pair 499 0.03743285685777664\n\nEpoch Loss pair 499 0.036022212356328964\n\nEpoch Loss pair 499 0.034668389707803726\n\nEpoch Loss pair 499 0.03336837515234947\n\nEpoch Loss pair 499 0.03211980313062668\n\nEpoch Loss pair 499 0.030919883400201797\n\nEpoch Loss pair 499 0.029766706749796867\n\nEpoch Loss pair 499 0.028659479692578316\n\nEpoch Loss pair 499 0.027595916762948036\n\nEpoch Loss pair 499 0.02657366544008255\n\nEpoch Loss pair 499 0.025592273101210594\n\nEpoch Loss pair 499 0.024648893624544144\n\nEpoch Loss pair 499 0.02374223992228508\n\nEpoch Loss pair 499 0.02287021279335022\n\nEpoch Loss pair 499 0.022031152620911598\n\nEpoch Loss pair 499 0.02122475951910019\n\nEpoch Loss pair 499 0.020449724048376083\n\nEpoch Loss pair 499 0.019704429432749748\n\nEpoch Loss pair 499 0.018987173214554787\n\nEpoch Loss pair 499 0.01829679310321808\n\nEpoch Loss pair 499 0.01763381063938141\n\nEpoch Loss pair 499 0.016995837911963463\n\nEpoch Loss pair 499 0.01638266257941723\n\nEpoch Loss pair 499 0.01579180732369423\n\nEpoch Loss pair 499 0.015223232097923756\n\nEpoch Loss pair 499 0.01467626541852951\n\nEpoch Loss pair 499 0.014149892143905163\n\nEpoch Loss pair 499 0.013643745332956314\n\nEpoch Loss pair 499 0.013156183063983917\n\nEpoch Loss pair 499 0.01268704142421484\n\nEpoch Loss pair 499 0.012235237285494804\n\nEpoch Loss pair 499 0.011800426058471203\n\nEpoch Loss pair 499 0.01138161588460207\n\nEpoch Loss pair 499 0.010978600941598415\n\nEpoch Loss pair 499 0.010590450838208199\n\nEpoch Loss pair 499 0.010216481052339077\n\nEpoch Loss pair 499 0.009856563992798328\n\nEpoch Loss pair 499 0.009509648196399212\n\nEpoch Loss pair 499 0.009175627492368221\n\nEpoch Loss pair499 0.008854025043547153\n\nEpoch Loss pair 499 0.008544247597455978\n\nEpoch Loss pair 499 0.008245434612035751\n\nEpoch Loss pair 499 0.007957644760608673\n\nEpoch Loss pair 499 0.007680418901145458\n\nEpoch Loss pair 499 0.007413465529680252\n\nEpoch Loss pair 499 0.007156442850828171\n\nEpoch Loss pair 499 0.006908326875418425\n\nEpoch Loss pair 499 0.006669133901596069\n\nEpoch Loss pair 499 0.006438644137233496\n\nEpoch Loss pair 499 0.006216550711542368\n\nEpoch Loss pair 499 0.006002519745379686\n\nEpoch Loss pair 499 0.0057963961735367775\n\nEpoch Loss pair 499 0.00559771154075861\n\nEpoch Loss pair 499 0.005406101234257221\n\nEpoch Loss pair 499 0.005221278872340918\n\nEpoch Loss pair 499 0.005043120589107275\n\nEpoch Loss pair 499 0.004871385637670755\n\nEpoch Loss pair 499 0.004705746192485094\n\nEpoch Loss pair 499 0.0045459517277777195\n\nEpoch Loss pair 499 0.004391741938889027\n\nEpoch Loss pair 499 0.0042430078610777855\n\nEpoch Loss pair 499 0.004099545534700155\n\nEpoch Loss pair 499 0.003961117006838322\n\nEpoch Loss pair 499 0.0038275232072919607\n\nEpoch Loss pair 499 0.003698692424222827\n\nEpoch Loss pair 499 0.003574344329535961\n\nEpoch Loss pair 499 0.0034542684443295\n\nEpoch Loss pair 499 0.0033384552225470543\n\nEpoch Loss pair 499 0.0032266597263514996\n\nEpoch Loss pair 499 0.003118830034509301\n\nEpoch Loss pair 499 0.0030147365760058165\n\nEpoch Loss pair 499 0.002914218930527568\n\nEpoch Loss pair 499 0.002817228902131319\n\nEpoch Loss pair 499 0.0027235609013587236\n\nEpoch Loss pair 499 0.002633118536323309\n\nEpoch Loss pair 499 0.0025458212476223707\n\nEpoch Loss pair 499 0.002461523748934269\n\nEpoch Loss pair 499 0.002380095887929201\n\nEpoch Loss pair 499 0.0023014729376882315\n\nEpoch Loss pair 499 0.0022255470976233482\n\nEpoch Loss pair 499 0.002152302535250783\n\nEpoch Loss pair 499 0.0020814654417335987\n\nEpoch Loss pair 499 0.002013100776821375\n\nEpoch Loss pair 499 0.0019470865372568369\n\nEpoch Loss pair 499 0.0018833738286048174\n\nEpoch Loss pair 499 0.0018217564793303609\n\nEpoch Loss pair 499 0.0017621897859498858\n\nEpoch Loss pair 499 0.0017046659486368299\n\nEpoch Loss pair 499 0.0016490865964442492\n\nEpoch Loss pair 499 0.0015953639522194862\n\nEpoch Loss pair 499 0.0015434576198458672\n\nEpoch Loss pair 499 0.0014933295315131545\n\nEpoch Loss pair 499 0.0014448537258431315\n\nEpoch Loss pair 499 0.0013980212388560176\n\nEpoch Loss pair 499 0.0013527936534956098\n\nEpoch Loss pair 499 0.001309062005020678\n\nEpoch Loss pair 499 0.0012668216368183494\n\nEpoch Loss pair 499 0.0012259491486474872\n\nEpoch Loss pair 499 0.0011864800471812487\n\nEpoch Loss pair 499 0.0011482773115858436\n\nEpoch Loss pair 499 0.0011113728396594524\n\nEpoch Loss pair 499 0.0010756880510598421\n\nEpoch Loss pair 499 0.0010411805706098676\n\nEpoch Loss pair 499 0.0010078372433781624\n\nEpoch Loss pair 499 0.0009756134240888059\n\nEpoch Loss pair 499 0.0009444544557482004\n\nEpoch Loss pair 499 0.0009143043425865471\n\nEpoch Loss pair 499 0.0008851294405758381\n\nEpoch Loss pair 499 0.0008569463971070945\n\nEpoch Loss pair 499 0.000829676107969135\n\nEpoch Loss pair 499 0.0008033085614442825\n\nEpoch Loss pair 499 0.0007778119761496782\n\nEpoch Loss pair 499 0.0007531650480814278\n\nEpoch Loss pair 499 0.0007292882073670626\n\nEpoch Loss pair 499 0.0007062168442644179\n\nEpoch Loss pair 499 0.0006838995032012463\n\nEpoch Loss pair 499 0.0006623091176152229\n\nEpoch Loss pair 499 0.0006414115778170526\n\nEpoch Loss pair 499 0.0006212110165506601\n\nEpoch Loss pair 499 0.0006016565021127462\n\nEpoch Loss pair 499 0.0005827320856042206\n\nEpoch Loss pair 499 0.0005644373595714569\n\nEpoch Loss pair 499 0.000546715222299099\n\nEpoch Loss pair 499 0.0005295880837365985\n\nEpoch Loss pair 499 0.0005130190402269363\n\nEpoch Loss pair 499 0.0004969720030203462\n\nEpoch Loss pair 499 0.00048143608728423715\n\nEpoch Loss pair 499 0.00046640675282105803\n\nEpoch Loss pair 499 0.00045188903459347785\n\nEpoch Loss pair 499 0.00043781884596683085\n\nEpoch Loss pair 499 0.0004242165887262672\n\nEpoch Loss pair 499 0.00041104096453636885\n\nEpoch Loss pair 499 0.00039828874287195504\n\nEpoch Loss pair 499 0.00038594871875829995\n\nEpoch Loss pair 499 0.00037400139262899756\n\nEpoch Loss pair 499 0.00036243180511519313\n\nEpoch Loss pair 499 0.0003512303519528359\n\nEpoch Loss pair 499 0.0003403848095331341\n\nEpoch Loss pair 499 0.00032989244209602475\n\nEpoch Loss pair 499 0.0003197280748281628\n\nEpoch Loss pair 499 0.0003098839661106467\n\nEpoch Loss pair 499 0.00030034835799597204\n\nEpoch Loss pair 499 0.0002911196497734636\n\nEpoch Loss pair 499 0.00028218026272952557\n\nEpoch Loss pair 499 0.0002735308080445975\n\nEpoch Loss pair 499 0.00026514832279644907\n\nEpoch Loss pair 499 0.00025702937273308635\n\nEpoch Loss pair 499 0.0002491712511982769\n\nEpoch Loss pair 499 0.00024155677238013595\n\nEpoch Loss pair 499 0.0002341799408895895\n\nEpoch Loss pair 499 0.00022703099239151925\n\nEpoch Loss pair 499 0.00022011435066815466\n\nEpoch Loss pair 499 0.00021340906096156687\n\nEpoch Loss pair 499 0.0002069168258458376\n\nEpoch Loss pair 499 0.00020062137627974153\n\nEpoch Loss pair 499 0.00019453215645626187\n\nEpoch Loss pair 499 0.00018863219884224236\n\nEpoch Loss pair 499 0.0001829098619055003\n\nEpoch Loss pair 499 0.0001773667027009651\n\nEpoch Loss pair 499 0.00017199694411829114\n\nEpoch Loss pair 499 0.000166795973200351\n\nEpoch Loss pair 499 0.000161759540787898\n\nEpoch Loss pair 499 0.00015687303675804287\n\nEpoch Loss pair 499 0.00015213819278869778\n\nEpoch Loss pair 499 0.00014755385927855968\n\nEpoch Loss pair 499 0.00014310909318737686\n\nEpoch Loss pair 499 0.00013880529149901122\n\nEpoch Loss pair 499 0.00013462828064803034\n\nEpoch Loss pair 499 0.00013058229524176568\n\nEpoch Loss pair 499 0.000126659419038333\n\nEpoch Loss pair 499 0.00012286145647522062\n\nEpoch Loss pair 499 0.0001191763803944923\n\nEpoch Loss pair 499 0.00011560327402548864\n\nEpoch Loss pair 499 0.00011214586265850812\n\nEpoch Loss pair 499 0.00010879281762754545\n\nEpoch Loss pair 499 0.00010554323671385646\n\nEpoch Loss pair 499 0.00010239116818411276\n\nEpoch Loss pair 499 9.933567343978211e-05\n\nEpoch Loss pair 499 9.63722268352285e-05\n\nEpoch Loss pair 499 9.35005082283169e-05\n\nEpoch Loss pair 499 9.07227658899501e-05\n\nEpoch Loss pair 499 8.80211591720581e-05\n\nEpoch Loss pair 499 8.540574344806373e-05\n\nEpoch Loss pair 499 8.28687334433198e-05\n\nEpoch Loss pair 499 8.040794637054205e-05\n\nEpoch Loss pair 499 7.802393520250916e-05\n\nEpoch Loss pair 499 7.570932211820036e-05\n\nEpoch Loss pair 499 7.346785423578694e-05\n\nEpoch Loss pair 499 7.129374716896564e-05\n\nEpoch Loss pair 499 6.918234430486336e-05\n\nEpoch Loss pair 499 6.713988113915548e-05\n\nEpoch Loss pair 499 6.515459244837984e-05\n\nEpoch Loss pair 499 6.323012348730117e-05\n\nEpoch Loss pair 499 6.136684532975778e-05\n\nEpoch Loss pair 499 5.955745655228384e-05\n\nEpoch Loss pair 499 5.7802797527983785e-05\n\nEpoch Loss pair 499 5.610074367723428e-05\n\nEpoch Loss pair 499 5.4449821618618444e-05\n\nEpoch Loss pair 499 5.285225415718742e-05\n\nEpoch Loss pair 499 5.1299324695719406e-05\n\nEpoch Loss pair 499 4.9792928621172905e-05\n\nEpoch Loss pair 499 4.833047205465846e-05\n\nEpoch Loss pair 499 4.691401045420207e-05\n\nEpoch Loss pair 499 4.5538839913206175e-05\n\nEpoch Loss pair 499 4.420543336891569e-05\n\nEpoch Loss pair 499 4.2911233322229236e-05\n\nEpoch Loss pair 499 4.1657458496047184e-05\n\nEpoch Loss pair 499 4.0440467273583636e-05\n\nEpoch Loss pair 499 3.9259666664293036e-05\n\nEpoch Loss pair 499 3.8113743357826024e-05\n\nEpoch Loss pair 499 3.7002591852797195e-05\n\nEpoch Loss pair 499 3.59235200448893e-05\n\nEpoch Loss pair 499 3.487733192741871e-05\n\nEpoch Loss pair 499 3.386301614227705e-05\n\nEpoch Loss pair 499 3.287723666289821e-05\n\nEpoch Loss pair 499 3.192177609889768e-05\n\nEpoch Loss pair 499 3.099549940088764e-05\n\nEpoch Loss pair 499 3.0093557143118232e-05\n\nEpoch Loss pair 499 2.9220193027867936e-05\n\nEpoch Loss pair 499 2.8373400709824637e-05\n\nEpoch Loss pair 499 2.755136119958479e-05\n\nEpoch Loss pair 499 2.6752222765935585e-05\n\nEpoch Loss pair 499 2.597808816062752e-05\n\nEpoch Loss pair 499 2.5226096113328822e-05\n\nEpoch Loss pair 499 2.4497103368048556e-05\n\nEpoch Loss pair 499 2.378897806920577e-05\n\nEpoch Loss pair 499 2.310011041117832e-05\n\nEpoch Loss pair 499 2.243441849714145e-05\n\nEpoch Loss pair 499 2.178774229832925e-05\n\nEpoch Loss pair 499 2.1158835807000287e-05\n\nEpoch Loss pair 499 2.0548915927065536e-05\n\nEpoch Loss pair 499 1.9956682081101462e-05\n\nEpoch Loss pair 499 1.9382894606678747e-05\n\nEpoch Loss pair 499 1.8824854123522528e-05\n\nEpoch Loss pair 499 1.8283324607182294e-05\n\nEpoch Loss pair 499 1.775784039637074e-05\n\nEpoch Loss pair 499 1.724828325677663e-05\n\nEpoch Loss pair 499 1.6753951058490202e-05\n\nEpoch Loss pair 499 1.6272661014227197e-05\n\nEpoch Loss pair 499 1.5806186638656072e-05\n\nEpoch Loss pair 499 1.535349656478502e-05\n\nEpoch Loss pair 499 1.4912935512256809e-05\n\nEpoch Loss pair 499 1.4486662621493451e-05\n\nEpoch Loss pair 499 1.4072666999709327e-05\n\nEpoch Loss pair 499 1.366993001283845e-05\n\nEpoch Loss pair 499 1.3280027815198991e-05\n\nEpoch Loss pair 499 1.2900672118121292e-05\n\nEpoch Loss pair 499 1.25322976600728e-05\n\nEpoch Loss pair 499 1.2174265975772869e-05\n\nEpoch Loss pair 499 1.1827278285636567e-05\n\nEpoch Loss pair 499 1.1490485121612437e-05\n\nEpoch Loss pair 499 1.1163338967890013e-05\n\nEpoch Loss pair 499 1.0845369615708478e-05\n\nEpoch Loss pair 499 1.0536505214986391e-05\n\nEpoch Loss pair 499 1.0236714842903893e-05\n\nEpoch Loss pair 499 9.945465535565745e-06\n\nEpoch Loss pair 499 9.663101991463918e-06\n\nEpoch Loss pair 499 9.388779290020466e-06\n\nEpoch Loss pair 499 9.122643859882373e-06\n\nEpoch Loss pair 499 8.864238225214649e-06\n\nEpoch Loss pair 499 8.612288183940109e-06\n\nEpoch Loss pair 499 8.367906048079021e-06\n\nEpoch Loss pair 499 8.131208232953213e-06\n\nEpoch Loss pair 499 7.901065146143083e-06\n\nEpoch Loss pair 499 7.6774103945354e-06\n\nEpoch Loss pair 499 7.459749213012401e-06\n\nEpoch Loss pair 499 7.249258942465531e-06\n\nEpoch Loss pair 499 7.043748155410867e-06\n\nEpoch Loss pair 499 6.844820745754987e-06\n\nEpoch Loss pair 499 6.6518705352791585e-06\n\nEpoch Loss pair 499 6.464150828833226e-06\n\nEpoch Loss pair 499 6.2815906858304515e-06\n\nEpoch Loss pair 499 6.104408839746611e-06\n\nEpoch Loss pair 499 5.93230015510926e-06\n\nEpoch Loss pair 499 5.76508045924129e-06\n\nEpoch Loss pair 499 5.603133104159497e-06\n\nEpoch Loss pair 499 5.4454685596283525e-06\n\nEpoch Loss pair 499 5.291931302053854e-06\n\nEpoch Loss pair 499 5.143536327523179e-06\n\nEpoch Loss pair 499 4.998840722691966e-06\n\nEpoch Loss pair 499 4.8586452976451255e-06\n\nEpoch Loss pair 499 4.721990990219638e-06\n\nEpoch Loss pair 499 4.589398940879619e-06\n\nEpoch Loss pair 499 4.459960564417997e-06\n\nEpoch Loss pair 499 4.3351064960006624e-06\n\nEpoch Loss pair 499 4.213899956084788e-06\n\nEpoch Loss pair 499 4.095565145689761e-06\n\nEpoch Loss pair 499 3.980698238592595e-06\n\nEpoch Loss pair 499 3.869760803354438e-06\n\nEpoch Loss pair 499 3.7617057841998758e-06\n\nEpoch Loss pair 499 3.656700982901384e-06\n\nEpoch Loss pair 499 3.5544621823646594e-06\n\nEpoch Loss pair 499 3.4558891002234304e-06\n\nEpoch Loss pair 499 3.3590667953831144e-06\n\nEpoch Loss pair 499 3.26595431943133e-06\n\nEpoch Loss pair 499 3.174900029989658e-06\n\nEpoch Loss pair 499 3.086802280449774e-06\n\nEpoch Loss pair 499 3.0010758109710878e-06\n\nEpoch Loss pair 499 2.917801111834706e-06\n\nEpoch Loss pair 499 2.8370984637149377e-06\n\nEpoch Loss pair 499 2.758057689788984e-06\n\nEpoch Loss pair 499 2.6816517220140668e-06\n\nEpoch Loss pair 499 2.606899897727999e-06\n\nEpoch Loss pair 499 2.534279019528185e-06\n\nEpoch Loss pair 499 2.4645873963891063e-06\n\nEpoch Loss pair 499 2.3961217721080175e-06\n\nEpoch Loss pair 499 2.330339839318185e-06\n\nEpoch Loss pair 499 2.2655933662463212e-06\n\nEpoch Loss pair 499 2.2028250441508135e-06\n\n"
    }
   ],
   "source": [
    "# Implement Network with Tensors + AutoGrad + nn Module + optim\n",
    "\n",
    "class TwoLayerNN(nn.Module):\n",
    "    def __init__(self, input_dims, hidden_dims, output_dims):\n",
    "        super(TwoLayerNN, self).__init__()\n",
    "        self.linear_one = nn.Linear(in_features=input_dims, out_features=hidden_dims)\n",
    "        self.linear_two = nn.Linear(in_features=hidden_dims, out_features=output_dims)\n",
    "    \n",
    "    def forward(self, input_tensor:torch.Tensor):\n",
    "        h_relu = self.linear_one(input_tensor).clamp(min=0)\n",
    "        y_pred = self.linear_two(h_relu)\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "d_type = torch.float\n",
    "gpu_device = torch.device(\"cuda:0\")\n",
    "\n",
    "# variables\n",
    "batch_size, input_dimensions, hidden_dimensions, output_dimensions = 64, 1000, 100, 10\n",
    "learning_rate = 1e-4\n",
    "# Create random input and output data\n",
    "random_input = torch.randn(batch_size, input_dimensions, device=gpu_device, dtype=d_type)\n",
    "random_output = torch.randn(batch_size, output_dimensions, device=gpu_device, dtype=d_type)\n",
    "\n",
    "# Use the nn package to define our model as a sequence of layers. nn.Sequential\n",
    "# is a Module which contains other Modules, and applies them in sequence to\n",
    "# produce its output. Each Linear Module computes output from input using a\n",
    "# linear function, and holds internal Tensors for its weight and bias.\n",
    "model = TwoLayerNN(input_dims=input_dimensions, hidden_dims=hidden_dimensions, output_dims=output_dimensions).to(gpu_device)\n",
    "\n",
    "loss_fxn = nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y by passing x to the model. Module objects\n",
    "    # override the __call__ operator so you can call them like functions. When\n",
    "    # doing so you pass a Tensor of input data to the Module and it produces\n",
    "    # a Tensor of output data.\n",
    "    y_pred = model(random_input)\n",
    "\n",
    "    # Compute and print loss. We pass Tensors containing the predicted and true\n",
    "    # values of y, and the loss function returns a Tensor containing the\n",
    "    # loss.\n",
    "    loss = loss_fxn(y_pred, random_output)\n",
    "    if idx % 100 == 99:\n",
    "        print(\"Epoch Loss pair\", idx, loss.item())\n",
    "        print()\n",
    "\n",
    "    # Zero the gradients before running the backward pass.\n",
    "    model.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to all the learnable\n",
    "    # parameters of the model. Internally, the parameters of each Module are stored\n",
    "    # in Tensors with requires_grad=True, so this call will compute gradients for\n",
    "    # all learnable parameters in the model.\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}