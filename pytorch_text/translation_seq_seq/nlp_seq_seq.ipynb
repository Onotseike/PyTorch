{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python361064bitpytorchpython36conda67ab338b5aea4512a55c45a4f56020b8",
   "display_name": "Python 3.6.10 64-bit ('pytorch_python_36': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the third and final tutorial on doing “NLP From Scratch”, where we write our own classes and functions to preprocess the data to do our NLP modeling tasks. We hope after you complete this tutorial that you’ll proceed to learn how torchtext can handle much of this preprocessing for you in the three tutorials immediately following this one.\n",
    "\n",
    "In this project we will be teaching a neural network to translate from French to English.\n",
    "\n",
    "This is made possible by the simple but powerful idea of the sequence to sequence network, in which two recurrent neural networks work together to transform one sequence to another. An encoder network condenses an input sequence into a vector, and a decoder network unfolds that vector into a new sequence.\n",
    "\n",
    "To improve upon this model we’ll use an attention mechanism, which lets the decoder learn to focus over a specific range of the input sequence.\n",
    "\n",
    "**Read about sequence to sequence Neural networks**"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import time\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as fxnl\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the character encoding used in the character-level RNN tutorials, we will be representing each word in a language as a one-hot vector, or giant vector of zeros except for a single one (at the index of the word). Compared to the dozens of characters that might exist in a language, there are many many more words, so the encoding vector is much larger. We will however cheat a bit and trim the data to only use a few thousand words per language.\n",
    "\n",
    "\n",
    "We’ll need a unique index per word to use as the inputs and targets of the networks later. To keep track of all this we will use a helper class called Lang which has word → index (word2index) and index → word (index2word) dictionaries, as well as a count of each word word2count to use to later replace rare words."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word_to_index = {}\n",
    "        self.word_to_count = {}\n",
    "        self.index_to_word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.num_words = 2\n",
    "    \n",
    "    def add_Sentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.add_Word(word)\n",
    "    \n",
    "    def add_Word(self, word):\n",
    "        if word not in self.word_to_index:\n",
    "            self.word_to_index[word] = self.num_words\n",
    "            self.word_to_count[word] = 1\n",
    "            self.index_to_word[self.num_words] = word\n",
    "            self.num_words += 1\n",
    "        else:\n",
    "            self.word_to_count[word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The files are all in Unicode, to simplify we will turn Unicode characters to ASCII, make everything lowercase, and trim most punctuation."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicode_to_ascii(unicode_str):\n",
    "    return ''.join(char for char in unicodedata.normalize('NFD', unicode_str) \n",
    "    if unicodedata.category(char) != 'Mn')\n",
    "\n",
    "def normalize_String(str_var):\n",
    "    str_var = unicode_to_ascii(str_var.lower().strip())\n",
    "    str_var = re.sub(r\"([.!?])\", r\" \\1\", str_var)\n",
    "    str_var = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", str_var)\n",
    "    return str_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read the data file we will split the file into lines, and then split lines into pairs. The files are all English → Other Language, so if we want to translate from Other Language → English I added the reverse flag to reverse the pairs."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_Langs(language_one, language_two, reverse=False):\n",
    "    print(\"Reading lines....... \")\n",
    "\n",
    "    lines = open('dataset/%s-%s.txt' % (language_one, language_two), encoding='utf-8').\\\n",
    "        read().strip().split('\\n')\n",
    "    \n",
    "    pairs = [[normalize_String(s) for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(pair)) for pair in pairs]\n",
    "        input_language = Lang(language_two)\n",
    "        output_language = Lang(language_one)\n",
    "    else:\n",
    "        input_language = Lang(language_one)\n",
    "        output_language = Lang(language_two)\n",
    "    \n",
    "    return input_language, output_language, pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are a lot of example sentences and we want to train something quickly, we’ll trim the data set to only relatively short and simple sentences. Here the maximum length is 10 words (that includes ending punctuation) and we’re filtering to sentences that translate to the form “I am” or “He is” etc. (accounting for apostrophes replaced earlier)."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 10\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s \",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_Pair(pair):\n",
    "    return len(pair[0].split(' ')) < MAX_LENGTH and \\\n",
    "        len(pair[1].split(' ')) < MAX_LENGTH and \\\n",
    "        pair[1].startswith(eng_prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_Pairs(pairs): return [pair for pair in pairs if filter_Pair(pair)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full process for preparing the data is:\n",
    "\n",
    "    * Read text file and split into lines, split lines into pairs\n",
    "    * Normalize text, filter by length and content\n",
    "    * Make word lists from sentences in pairs"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_Data(language_one, language_two, reverse=False):\n",
    "    input_language, output_language, pairs = read_Langs(language_one=language_one,     language_two=language_two, reverse=reverse)\n",
    "\n",
    "    print(\"+\" * 89)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    print(\"+\" * 89)\n",
    "\n",
    "    pairs = filter_Pairs(pairs)\n",
    "\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"+\" * 89)\n",
    "    print(\"Counting words...\")\n",
    "    print(\"+\" * 89)\n",
    "\n",
    "    for pair in pairs:\n",
    "        input_language.add_Sentence(pair[0])\n",
    "        output_language.add_Sentence(pair[1])\n",
    "    \n",
    "    print(\"Counted words:\")\n",
    "    print(\"-\" * 89)\n",
    "    print(input_language.name, \"->\", input_language.num_words)\n",
    "    print(\"-\" * 89)\n",
    "    print(output_language.name, \"->\", output_language.num_words)\n",
    "\n",
    "    return input_language, output_language, pairs\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "ique .', 'he is in excellent physical condition .'], ['il projette de lancer son entreprise .', 'he is planning to launch his business .'], ['il est cale en litterature anglaise .', 'he is well read in english literature .'], ['il travaille dans le domaine de la biologie .', 'he is working in the field of biology .'], ['il est un celebre chanteur populaire au japon .', 'he s a famous popular singer in japan .'], ['c est une pointure dans son domaine .', 'he s a leading authority in his field .'], ['c est une sommite dans son domaine .', 'he s a leading authority in his field .'], ['il est etudiant en litterature japonaise .', 'he s a student of japanese literature .'], ['c est un jeune adolescent impressionnable .', 'he s a young impressionable teenager .'], ['il depend totalement de ses parents .', 'he s totally dependent on his parents .'], ['je suis libre jusqu a heures ce soir .', 'i am free till o clock this evening .'], ['je vais ecrire une lettre demain .', 'i am going to write a letter tomorrow .'], ['ces nouvelles chaussures sont inconfortables .', 'i am uncomfortable in these new shoes .'], ['je suis tres interesse par des histoires .', 'i am very interested in these stories .'], ['j ai presque fini de lire ce livre .', 'i m almost finished reading this book .'], ['j ai honte car j ai agi stupidement .', 'i m ashamed because i acted foolishly .'], ['je commence a avoir un peu sommeil .', 'i m beginning to feel a little sleepy .'], ['je vais jouer au tennis ce soir .', 'i m going to play tennis this evening .'], ['toutes ces disputes commencent a me fatiguer .', 'i m growing tired of all this arguing .'], ['j ecoute la derniere chanson de bjork .', 'i m listening to bjork s latest song .'], ['je suis plus interesse par l anglais parle .', 'i m more interested in spoken english .'], ['je ne vais plus etudier le francais .', 'i m not going to study french anymore .'], ['je fait maintenant officiellement partie de ce groupe .', 'i m now officially part of this group .'], ['je t ordonne de partir immediatement .', 'i m ordering you to leave immediately .'], ['je vous ordonne de partir immediatement .', 'i m ordering you to leave immediately .'], ['je suis sure que les enfants grandissent .', 'i m sure the children are getting big .'], ['les etudes sociales m interessent beaucoup .', 'i m very interested in social studies .'], ['j attends un appel tres important .', 'i m waiting for a very important call .'], ['elle est toujours en quete de compliments .', 'she is always fishing for compliments .'], ['elle rassemble du materiel pour un livre .', 'she is collecting material for a book .'], ['elle connait bien l histoire du japon .', 'she is familiar with japanese history .'], ['ils sont satisfaits de la nouvelle maison .', 'they are satisfied with the new house .'], ['ils s interessent beaucoup a l astronomie .', 'they are very interested in astronomy .'], ['ils sont tres interesses par l astronomie .', 'they are very interested in astronomy .'], ['nous sommes tous convaincus de son innocence .', 'we are all convinced of his innocence .'], ['nous sommes toutes convaincues de son innocence .', 'we are all convinced of his innocence .'], ['nous vous souhaitons un prompt retablissement .', 'we are hoping for your quick recovery .'], ['nous sur pechons dans les oceans du monde .', 'we are overfishing the world s oceans .'], ['nous avons presque fini pour aujourd hui .', 'we re just about finished for the day .'], ['tu es responsable de cet accident .', 'you are responsible for this accident .'], ['il est membre du conseil paroissial .', 'he is a member of the parish committee .'], ['il est habitue a parler en public .', 'he is accustomed to speaking in public .'], ['il trouve toujours a redire aux autres .', 'he is always finding fault with others .'], ['il est normalement chez lui le soir .', 'he is generally at home in the evening .'], ['il fait de gros progres en anglais .', 'he is making great progress in english .'], ['il prevoit de developper son affaire .', 'he is planning to develop his business .'], ['il souffre d une maladie grave .', 'he is suffering from a serious illness .'], ['il est le chef du departement de vente .', 'he is the head of the sales department .'], ['c est le portrait crache de son pere .', 'he is the spitting image of his father .'], ['il depend completement de ses parents .', 'he is totally dependent on his parents .'], ['il est professeur de biologie a harvard .', 'he s a professor of biology at harvard .'], ['il se plaint toujours de la nourriture .', 'he s always complaining about the food .'], ['il est divorce depuis deux ans deja .', 'he s been divorced for years already .'], ['il depend financierement de son epouse .', 'he s financially dependent on his wife .'], ['je suis finlandais mais je parle aussi suedois .', 'i am finnish but i speak also swedish .'], ['je vais chez le dentiste demain .', 'i am going to see the dentist tomorrow .'], ['je te suis reconnaissant pour ta gentillesse .', 'i am grateful to you for your kindness .'], ['je vous suis reconnaissant de votre gentillesse .', 'i am grateful to you for your kindness .'], ['je suis capable de prendre mes propres decisions .', 'i m capable of making my own decisions .'], ['je suis capable de faire mes propres choix .', 'i m capable of making my own decisions .'], ['je vais rejoindre l orchestre de l ecole .', 'i m going to join the school orchestra .'], ['ma journee est tres dure aujourd hui .', 'i m having a very difficult time today .'], ['j ai hate de te voir bientot .', 'i m looking forward to seeing you soon .'], ['je suis impatient de te voir bientot .', 'i m looking forward to seeing you soon .'], ['je suis impatiente de te voir bientot .', 'i m looking forward to seeing you soon .'], ['je suis vraiment concerne par votre avenir .', 'i m really concerned about your future .'], ['c est une etudiante qui etudie serieusement .', 'she is a student who studies very hard .'], ['elle est completement sourde de l oreille gauche .', 'she is completely deaf in her left ear .'], ['elle est determinee a quitter l entreprise .', 'she is determined to leave the company .'], ['elle vit au milieu de nulle part .', 'she is living in the middle of nowhere .'], ['elle est assez grande pour voyager toute seule .', 'she is old enough to travel by herself .'], ['aujourd hui elle va beaucoup mieux qu hier .', 'she s much better today than yesterday .'], ['elle souffre d une maladie grave .', 'she s suffering from a serious disease .'], ['elle est assez jeune pour etre ta fille .', 'she s young enough to be your daughter .'], ['ils travaillent tous les deux a l animalerie .', 'they are both working at the pet store .'], ['ils sont actuellement en reunion .', 'they are currently attending a meeting .'], ['ils etudient le francais et le webdesign .', 'they re studying french and web design .'], ['nous attendons les vacances avec impatience .', 'we are looking forward to the holidays .'], ['on apprend encore a se connaitre .', 'we re still getting to know each other .'], ['vous vous amusez n est ce pas ?', 'you re enjoying yourselves aren t you ?'], ['tu as tres sommeil non ?', 'you re feeling very sleepy aren t you ?'], ['vous avez tres sommeil non ?', 'you re feeling very sleepy aren t you ?'], ['tu es le seul a me comprendre .', 'you re the only one who understands me .'], ['tu es la seule a me comprendre .', 'you re the only one who understands me .'], ['vous etes le seul a me comprendre .', 'you re the only one who understands me .'], ['vous etes la seule a me comprendre .', 'you re the only one who understands me .'], ['tu es trop suspicieux de tout .', 'you re too suspicious about everything .'], ['tu es trop suspicieuse de tout .', 'you re too suspicious about everything .'], ['vous etes trop suspicieux de tout .', 'you re too suspicious about everything .'], ['vous etes trop suspicieuses de tout .', 'you re too suspicious about everything .'], ['il est desormais dans une situation tres difficile .', 'he is now in a very difficult situation .'], ['il se plaint toujours de quelque chose .', 'he s always complaining about something .'], ['je suis loin d etre satisfait du resultat .', 'i am far from satisfied with the result .'], ['je pense partir a la montagne .', 'i am thinking of going to the mountains .'], ['j attends de vos nouvelles .', 'i m looking forward to hearing from you .'], ['j espere avoir de tes nouvelles .', 'i m looking forward to hearing from you .'], ['j attends avec impatience de vous revoir .', 'i m looking forward to seeing you again .'], ['je me rejouis de vous voir danser .', 'i m looking forward to seeing you dance .'], ['je suis impatient de te voir danser .', 'i m looking forward to seeing you dance .'], ['je suis impatiente de te voir danser .', 'i m looking forward to seeing you dance .'], ['j en ai marre d ecouter ses plaintes .', 'i m sick of listening to her complaints .'], ['je suis le second de trois enfants .', 'i m the second oldest of three children .'], ['j en ai assez d ecouter tes rodomontades .', 'i m tired of listening to your bragging .'], ['elle est interessee a apprendre de nouvelles idees .', 'she is interested in learning new ideas .'], ['elle chante les derniers tubes .', 'she is singing the latest popular songs .'], ['elle chante les dernieres chansons populaires .', 'she is singing the latest popular songs .'], ['elle se plaint toujours de la nourriture .', 'she s always complaining about the food .'], ['le nom de l auteur nous est familier .', 'we are familiar with that author s name .'], ['nous mesurons la profondeur de la riviere .', 'we are measuring the depth of the river .'], ['nous travaillons pour la paix .', 'we are working in the interest of peace .'], ['nous avons presque acheve ce travail .', 'we re just about finished with this job .'], ['tu me tiens la main sur la photo .', 'you are holding my hand in that picture .'], ['fais ce qu il te plait .', 'you are welcome to do anything you like .'], ['tu ne regardes pas tout le paysage .', 'you re not looking at the whole picture .'], ['vous ne regardez pas l ensemble du tableau .', 'you re not looking at the whole picture .'], ['vous tirez avantage de sa faiblesse .', 'you re taking advantage of her weakness .'], ['tu tires avantage de sa faiblesse .', 'you re taking advantage of her weakness .'], ['il est diplomate a l ambassade americaine .', 'he is a diplomat at the american embassy .'], ['il est un critique des autres tres irritant .', 'he is a very irritating critic of others .'], ['il essaye toujours de faire l impossible .', 'he is always trying to do the impossible .'], ['il ne se presente pas aux prochaines elections .', 'he is not running in the coming election .'], ['il recupere lentement de sa maladie .', 'he is slowly recovering from his illness .'], ['il etudie l histoire a l universite .', 'he is studying history at the university .'], ['il est toujours ponctuel pour ses rendez vous .', 'he s always on time for his appointments .'], ['il est tres fier de sa moto trafiquee .', 'he s very proud of his custom motorcycle .'], ['il s inquiete de sa calvitie naissante .', 'he s worried about his receding hairline .'], ['je suis confronte a un probleme difficile .', 'i am confronted with a difficult problem .'], ['je me rejouis de te revoir .', 'i am looking forward to seeing you again .'], ['je me rejouis de vous revoir .', 'i am looking forward to seeing you again .'], ['je suis impatient de vous revoir .', 'i am looking forward to seeing you again .'], ['je suis impatiente de vous revoir .', 'i am looking forward to seeing you again .'], ['je suis impatient de te revoir .', 'i am looking forward to seeing you again .'], ['je suis impatiente de te revoir .', 'i am looking forward to seeing you again .'], ['je songe a acheter un nouveau parasol .', 'i am thinking about buying a new parasol .'], ['je suis considere comme traitre a ce pays .', 'i m considered a traitor to this country .'], ['j adore ecouter de la musique classique .', 'i m fond of listening to classical music .'], ['je suis fatigue d ecouter tes plaintes .', 'i m sick of listening to your complaints .'], ['je suis heureux de vous avoir rencontre .', 'i m very happy to make your acquaintance .'], ['elle est extremement sensible au froid .', 'she is exceedingly sensitive to the cold .'], ['elle s entraine au piano nuit et jour .', 'she s practicing the piano day and night .'], ['ils expriment leur amour en s enlacant .', 'they re expressing their love by hugging .'], ['on nous encourage a utiliser notre imagination .', 'we are encouraged to use our imagination .'], ['la moisson s annonce bonne cette annee .', 'we re expecting a good harvest this year .'], ['nous nous rejouissons de votre presence .', 'we re looking forward to your being here .'], ['nous sommes tres reconnaissants pour votre hospitalite .', 'we re very grateful for your hospitality .'], ['tu reussiras sans aucun doute quoique tu fasses .', 'you are sure to succeed whatever you do .'], ['vous etes toujours en desaccord avec votre patron .', 'you re always disagreeing with your boss .'], ['tu n es pas vraiment millionnaire si ?', 'you re not really a millionaire are you ?'], ['vous n etes pas vraiment millionnaire si ?', 'you re not really a millionaire are you ?'], ['il est docteur et professeur d universite .', 'he is a doctor and a university professor .'], ['il est completement absorbe par ses affaires .', 'he is completely absorbed in his business .'], ['je suis sur que vos pensees sont nobles .', 'i am certain that you have noble thoughts .'], ['j oublie toujours les noms des gens .', 'i am constantly forgetting people s names .'], ['j attends de toi un travail serieux .', 'i am expecting some serious work from you .'], ['je suis impatient de voir ton pere .', 'i m looking forward to seeing your father .'], ['c est une personne particulierement interessante .', 'she is a particularily interesting person .'], ['elle est plus une relation qu une amie .', 'she is more an acquaintance than a friend .'], ['elle est non seulement intelligente mais aussi jolie .', 'she is not only intelligent but beautiful .'], ['ils sculptent une statue de marbre .', 'they are chiseling a statue out of marble .'], ['ce sont des affaires dont nous devons parler .', 'they are matters which we need to discuss .'], ['ils emmenent marie a la salle des urgences .', 'they re taking mary to the emergency room .'], ['nous nous rejouissons de votre venue .', 'we are all looking forward to your coming .'], ['nous faisons des affaires avec de nombreux pays .', 'we are doing business with many countries .'], ['nous sommes en affaires avec de nombreux pays .', 'we are doing business with many countries .'], ['nous allons voyager a l etranger cet ete .', 'we are going to travel abroad this summer .'], ['nous sommes impatients de vous voir .', 'we are looking forward to seeing you soon .'], ['ces problemes ne nous menent a rien .', 'we re getting nowhere with those problems .'], ['tu rales toujours apres quelque chose .', 'you re always complaining about something .'], ['tu es plus belle que dans mon souvenir .', 'you re more beautiful than i remember you .'], ['il se mele toujours de notre conversation .', 'he s always breaking into our conversation .'], ['j ecris pour exprimer mon insatisfaction .', 'i am writing to express my dissatisfaction .'], ['je romps ce soir avec ma petite amie .', 'i m breaking up with my girlfriend tonight .'], ['je romps ce soir avec ma petite copine .', 'i m breaking up with my girlfriend tonight .'], ['je romps ce soir avec ma nana .', 'i m breaking up with my girlfriend tonight .'], ['je suis financierement independant de mes parents .', 'i m economically independent of my parents .'], ['je pense serieusement a demenager a boston .', 'i m seriously considering moving to boston .'], ['je suis le porte parole de cette organisation .', 'i m the spokesperson for this organization .'], ['je suis le porte parole de cette institution .', 'i m the spokesperson for this organization .'], ['ils parlent de ce qu ils vont chanter .', 'they are talking about what they will sing .'], ['ils sont disposes a discuter du probleme .', 'they are willing to talk about the problem .'], ['elles sont disposees a discuter du probleme .', 'they are willing to talk about the problem .'], ['nous nous rejouissons de vous revoir .', 'we are looking forward to seeing you again .'], ['nous avons des invites ce soir .', 'we re having some guests over this evening .'], ['il est une autorite reconnue sur le sujet .', 'he is a recognized authority on the subject .'], ['je suis inquiet de votre attitude irresponsable .', 'i am alarmed by your irresponsible attitude .'], ['je suis etonne par ton attitude irresponsable .', 'i am alarmed by your irresponsible attitude .'], ['je pense cloturer mon compte d epargne .', 'i am thinking of closing my savings account .'], ['j ai hate de recevoir ta reponse .', 'i m looking forward to receiving your reply .'], ['il me tarde de recevoir votre reponse .', 'i m looking forward to receiving your reply .'], ['nous avons des problemes avec notre nouveau voisin .', 'we are having trouble with our new neighbor .'], ['nous discuterons du probleme demain .', 'we re going to discuss the problem tomorrow .'], ['il trouve toujours a redire aux autres .', 'he is always finding fault with other people .'], ['il s amuse en jouant aux jeux videos .', 'he is amusing himself by playing video games .'], ['il quitte narita pour hawaii ce soir .', 'he is leaving narita for hawaii this evening .'], ['il est competent en espagnol et en italien .', 'he is proficient in both spanish and italian .'], ['il bataille encore avec les croyances religieuses .', 'he is still grappling with religious beliefs .'], ['il est expert en litterature francaise .', 'he is well acquainted with french literature .'], ['c est un interprete dans une banque internationale .', 'he s an interpreter in an international bank .'], ['je lui dis constamment de bien se comporter .', 'i m constantly telling her to behave herself .'], ['elle trouve toujours des defauts a son mari .', 'she is always finding fault with her husband .'], ['elle est forte pour inventer des histoires interessantes .', 'she is good at making up interesting stories .'], ['nous attendons la publication de son ouvrage .', 'we are expecting the publication of his book .'], ['vous etes trop critique des defauts des autres .', 'you are too critical of others shortcomings .'], ['tu es trop critique des defauts des autres .', 'you are too critical of others shortcomings .'], ['vous etes probablement trop jeune pour le comprendre .', 'you re probably too young to understand this .'], ['tu es probablement trop jeune pour le comprendre .', 'you re probably too young to understand this .'], ['je pense apprendre le coreen le semestre prochain .', 'i m thinking of learning korean next semester .'], ['elle trouve toujours a redire aux autres .', 'she is always finding fault with other people .'], ['elle est tres receptive a la suggestion hypnotique .', 'she s very susceptible to hypnotic suggestion .'], ['nous enquetons sur le meurtre de tom jackson .', 'we re investigating the murder of tom jackson .'], ['elle est non seulement belle mais aussi intelligente .', 'she is not only beautiful but also intelligent .'], ['il entreprend des experiences dans son laboratoire .', 'he is carrying out experiments in his laboratory .'], ['j ai quelques difficultes a compiler ce programme .', 'i m having some problems compiling this software .'], ['ils collectent des dons pour l eglise .', 'they are collecting contributions for the church .'], ['il est un des candidats aux presidentielles americaines .', 'he is one of the american presidential candidates .']] \n\n-----------------------------------------------------------------------------------------\n['tu es trop maigre .', 'you re too skinny .']\n"
    }
   ],
   "source": [
    "input_language, output_language, pairs = prepare_Data(\"eng\", \"fra\", True)\n",
    "print(\"-\" * 89)\n",
    "print(input_language, \"\\n\")\n",
    "print(\"-\" * 89)\n",
    "print(output_language, \"\\n\")\n",
    "print(\"-\" * 89)\n",
    "print(pairs, \"\\n\")\n",
    "print(\"-\" * 89)\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Recurrent Neural Network, or RNN, is a network that operates on a sequence and uses its own output as input for subsequent steps.\n",
    "\n",
    "A Sequence to Sequence network, or seq2seq network, or Encoder Decoder network, is a model consisting of two RNNs called the encoder and decoder. The encoder reads an input sequence and outputs a single vector, and the decoder reads that vector to produce an output sequence.\n",
    "\n",
    "\n",
    "\n",
    "Unlike sequence prediction with a single RNN, where every input corresponds to an output, the seq2seq model frees us from sequence length and order, which makes it ideal for translation between two languages.\n",
    "\n",
    "Consider the sentence “Je ne suis pas le chat noir” → “I am not the black cat”. Most of the words in the input sentence have a direct translation in the output sentence, but are in slightly different orders, e.g. “chat noir” and “black cat”. Because of the “ne/pas” construction there is also one more word in the input sentence. It would be difficult to produce a correct translation directly from the sequence of input words.\n",
    "\n",
    "With a seq2seq model the encoder creates a single vector which, in the ideal case, encodes the “meaning” of the input sequence into a single vector — a single point in some N dimensional space of sentences.\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Encoder\n",
    "The encoder of a seq2seq network is a RNN that outputs some value for every word from the input sentence. For every input word the encoder outputs a vector and a hidden state, and uses the hidden state for the next input word."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size,  hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "    \n",
    "    def forward(self, input_tensor, hidden_tensor):\n",
    "        embedded = self.embedding(input_tensor).view(1, 1, -1)\n",
    "        output_tensor = embedded\n",
    "        output_tensor, hidden_tensor = self.gru(output_tensor, hidden_tensor)\n",
    "        return output_tensor, hidden_tensor\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Decoder\n",
    "\n",
    "The decoder is another RNN that takes the encoder output vector(s) and outputs a sequence of words to create the translation.\n",
    "\n",
    "Simple Decoder\n",
    "In the simplest seq2seq decoder we use only last output of the encoder. This last output is sometimes called the context vector as it encodes context from the entire sequence. This context vector is used as the initial hidden state of the decoder.\n",
    "\n",
    "At every step of decoding, the decoder is given an input token and hidden state. The initial input token is the start-of-string <SOS> token, and the first hidden state is the context vector (the encoder’s last hidden state)."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    def forward(self, input_tensor, hidden_tensor):\n",
    "        output_tensor = self.embedding(input_tensor).view(1, 1, -1)\n",
    "        output_tensor = fxnl.relu(output_tensor)\n",
    "        output_tensor, hidden_tensor = self.gru(output_tensor, hidden_tensor)\n",
    "        output_tensor = self.softmax(self.out(output_tensor[0]))\n",
    "        return output_tensor, hidden_tensor\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I encourage you to train and observe the results of this model, but to save space we’ll be going straight for the gold and introducing the Attention Mechanism.\n",
    "\n",
    "Attention Decoder\n",
    "If only the context vector is passed betweeen the encoder and decoder, that single vector carries the burden of encoding the entire sentence.\n",
    "\n",
    "Attention allows the decoder network to “focus” on a different part of the encoder’s outputs for every step of the decoder’s own outputs. First we calculate a set of attention weights. These will be multiplied by the encoder output vectors to create a weighted combination. The result (called attn_applied in the code) should contain information about that specific part of the input sequence, and thus help the decoder choose the right output words.\n",
    "\n",
    "\n",
    "Calculating the attention weights is done with another feed-forward layer attn, using the decoder’s input and hidden state as inputs. Because there are sentences of all sizes in the training data, to actually create and train this layer we have to choose a maximum sentence length (input length, for encoder outputs) that it can apply to. Sentences of the maximum length will use all the attention weights, while shorter sentences will only use the first few."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttentionDecoderRNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "    \n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = fxnl.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = fxnl.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = fxnl.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training\n",
    "\n",
    "Preparing Training Data\n",
    "\n",
    "To train, for each pair we will need an input tensor (indexes of the words in the input sentence) and target tensor (indexes of the words in the target sentence). While creating these vectors we will append the EOS token to both sequences."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexes_from_Sentence(lanuage, sentence):    \n",
    "    return [lanuage.word_to_index[word] for word in sentence.split(' ')]\n",
    "\n",
    "def tensor_from_Sentence(lanuage, sentence):\n",
    "    indexes = indexes_from_Sentence(lanuage=lanuage, sentence=sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "def tensor_from_Pair(pair):\n",
    "    input_tensor = tensor_from_Sentence(input_language, pair[0])\n",
    "    target_tensor = tensor_from_Sentence(output_language, pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the Model\n",
    "\n",
    "To train we run the input sentence through the encoder, and keep track of every output and the latest hidden state. Then the decoder is given the <SOS> token as its first input, and the last hidden state of the encoder as its first hidden state.\n",
    "\n",
    "“Teacher forcing” is the concept of using the real target outputs as each next input, instead of using the decoder’s guess as the next input. Using teacher forcing causes it to converge faster but when the trained network is exploited, it may exhibit instability.\n",
    "\n",
    "You can observe outputs of teacher-forced networks that read with coherent grammar but wander far from the correct translation - intuitively it has learned to represent the output grammar and can “pick up” the meaning once the teacher tells it the first few words, but it has not properly learned how to create the sentence from the translation in the first place.\n",
    "\n",
    "Because of the freedom PyTorch’s autograd gives us, we can randomly choose to use teacher forcing or not with a simple if statement. Turn teacher_forcing_ratio up to use more of it."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.init_hidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for e_idx in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_tensor[e_idx], encoder_hidden)\n",
    "        encoder_outputs[e_idx] = encoder_output[0, 0]\n",
    "    \n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if  random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        for d_idx in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            \n",
    "            loss += criterion(decoder_output, target_tensor[d_idx])\n",
    "            decoder_input = target_tensor[d_idx]\n",
    "    \n",
    "    else:\n",
    "        for d_idx in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            \n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[d_idx])\n",
    "            if decoder_input.item() == EOS_token: break\n",
    "\n",
    "    \n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def as_Minutes(seconds):\n",
    "    min = math.floor(seconds / 60)\n",
    "    seconds -= min * 60\n",
    "    return '%dm %ds' % (min, seconds)\n",
    "\n",
    "def time_since(since, percent):\n",
    "    now = time.time()\n",
    "    seconds = now - since\n",
    "    es = seconds / (percent)\n",
    "    rs = es - seconds\n",
    "    return '%s (- %s)' % (as_Minutes(seconds), as_Minutes(rs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The whole training process looks like this:\n",
    "\n",
    "\n",
    "Start a timer\n",
    "Initialize optimizers and criterion\n",
    "Create set of training pairs\n",
    "Start empty losses array for plotting\n",
    "Then we call train many times and occasionally print the progress (% of examples, time so far, estimated time) and average loss."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_Iters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0 \n",
    "    plot_loss_total = 0\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    training_pairs = [tensor_from_Pair(random.choice(pairs)) for i in range(n_iters)]\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "\n",
    "        loss = train(input_tensor=input_tensor, target_tensor=target_tensor, encoder=encoder,\n",
    "        decoder=decoder, encoder_optimizer=encoder_optimizer, decoder_optimizer=decoder_optimizer,\n",
    "        criterion=criterion)\n",
    "\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0 \n",
    "            print('%s (%d %d%%) %.4f' % (time_since(start, iter / n_iters), iter, \n",
    "            iter / n_iters * 100, print_loss_avg))\n",
    "        \n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0 \n",
    "    \n",
    "    show_Plot(plot_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting results\n",
    "\n",
    "\n",
    "Plotting is done with matplotlib, using the array of loss values plot_losses saved while training."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_Plot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation\n",
    "\n",
    "Evaluation is mostly the same as training, but there are no targets so we simply feed the decoder’s predictions back to itself for each step. Every time it predicts a word we add it to the output string, and if it predicts the EOS token we stop there. We also store the decoder’s attention outputs for display later."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter('./runs/nlp_seq_seq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensor_from_Sentence(input_language, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.init_hidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        writer.add_graph(encoder, (input_tensor[0], encoder_hidden))\n",
    "        writer.close()\n",
    "\n",
    "        \n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "            \n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "        writer.add_graph(decoder, (decoder_input, decoder_hidden, encoder_outputs))\n",
    "        writer.close()\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_attentions[di] = decoder_attention.data\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "           \n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_language.index_to_word[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words, decoder_attentions[:di + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_Randomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and Evaluating\n",
    "\n",
    "With all these helper functions in place (it looks like extra work, but it makes it easier to run multiple experiments) we can actually initialize a network and start training.\n",
    "\n",
    "Remember that the input sentences were heavily filtered. For this small dataset we can use relatively small networks of 256 hidden nodes and a single GRU layer. After about 40 minutes on a MacBook CPU we’ll get some reasonable results.\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "5m 49s (- 81m 28s) (5000 6%) 2.8677\n11m 22s (- 73m 53s) (10000 13%) 2.2908\n16m 49s (- 67m 16s) (15000 20%) 1.9334\n22m 14s (- 61m 10s) (20000 26%) 1.7582\n27m 43s (- 55m 26s) (25000 33%) 1.5405\n33m 6s (- 49m 39s) (30000 40%) 1.3675\n38m 41s (- 44m 13s) (35000 46%) 1.2223\n44m 2s (- 38m 32s) (40000 53%) 1.0970\n49m 30s (- 33m 0s) (45000 60%) 1.0157\n54m 59s (- 27m 29s) (50000 66%) 0.9003\n60m 27s (- 21m 59s) (55000 73%) 0.8039\n65m 56s (- 16m 29s) (60000 80%) 0.7315\n71m 27s (- 10m 59s) (65000 86%) 0.6986\n77m 0s (- 5m 30s) (70000 93%) 0.6184\n82m 46s (- 0m 0s) (75000 100%) 0.5517\n"
    }
   ],
   "source": [
    "hidden_size = 256\n",
    "encoder1 = EncoderRNN(input_language.num_words, hidden_size).to(device)\n",
    "attn_decoder1 = AttentionDecoderRNN(hidden_size, output_language.num_words, dropout_p=0.1).to(device)\n",
    "\n",
    "train_Iters(encoder1, attn_decoder1, 75000, print_every=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing Attention\n",
    "\n",
    "A useful property of the attention mechanism is its highly interpretable outputs. Because it is used to weight specific encoder outputs of the input sequence, we can imagine looking where the network is focused most at each time step.\n",
    "\n",
    "You could simply run plt.matshow(attentions) to see attention output displayed as a matrix, with the columns being input steps and rows being output steps:"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<matplotlib.image.AxesImage at 0x267ff65de10>"
     },
     "metadata": {},
     "execution_count": 65
    }
   ],
   "source": [
    "output_words, attentions = evaluate(\n",
    "    encoder1, attn_decoder1, \"je suis trop froid .\")\n",
    "plt.matshow(attentions.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "input = elle a cinq ans de moins que moi .\noutput = she is five years younger than me . <EOS>\ninput = elle est trop petit .\noutput = she s too short . <EOS>\ninput = je ne crains pas de mourir .\noutput = i m not scared of dying . <EOS>\ninput = c est un jeune directeur plein de talent .\noutput = he s a talented young director . <EOS>\n"
    }
   ],
   "source": [
    "def show_Attention(input_sentence, output_words, attentions):\n",
    "    # Set up figure with colorbar\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attentions.numpy(), cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Set up axes\n",
    "    ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
    "                       ['<EOS>'], rotation=90)\n",
    "    ax.set_yticklabels([''] + output_words)\n",
    "\n",
    "    # Show label at every tick\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def evaluate_And_Show_Attention(input_sentence):\n",
    "    output_words, attentions = evaluate(\n",
    "        encoder1, attn_decoder1, input_sentence)\n",
    "    print('input =', input_sentence)\n",
    "    print('output =', ' '.join(output_words))\n",
    "    show_Attention(input_sentence, output_words, attentions)\n",
    "\n",
    "\n",
    "evaluate_And_Show_Attention(\"elle a cinq ans de moins que moi .\")\n",
    "\n",
    "evaluate_And_Show_Attention(\"elle est trop petit .\")\n",
    "\n",
    "evaluate_And_Show_Attention(\"je ne crains pas de mourir .\")\n",
    "\n",
    "evaluate_And_Show_Attention(\"c est un jeune directeur plein de talent .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(encoder1.state_dict(), './model/encoder.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(attn_decoder1.state_dict(), './model/attention_decoder.pth')"
   ]
  }
 ]
}